# train_existing_mlp.py
"""
ê¸°ì¡´ ìˆ˜ì§‘ëœ ë°ì´í„°ë¡œ MLP ëª¨ë¸ í•™ìŠµ
merge_existing_data.pyë¡œ ë³‘í•©ëœ ë°ì´í„° ì‚¬ìš©

Author: AIoT Project Team
Date: 2024
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import os
import time
import json
from collections import Counter
from datetime import datetime

# =============================================================================
# ì„¤ì • ë° ìƒìˆ˜
# =============================================================================

# ì œìŠ¤ì²˜ ë¼ë²¨ ë§¤í•‘ (ê¸°ì¡´ ë°ì´í„°ì™€ ë™ì¼)
GESTURE_LABELS = {
    'four': 0, 'horizontal_V': 1, 'ok': 2, 'one': 3, 'promise': 4,
    'small_heart': 5, 'spider_man': 6, 'three': 7, 'three2': 8, 
    'thumbs_down': 9, 'thumbs_left': 10, 'thumbs_right': 11, 
    'thumbs_up': 12, 'two': 13, 'vertical_V': 14, 'nothing': 15
}

LABEL_TO_NAME = {v: k for k, v in GESTURE_LABELS.items()}

# í•™ìŠµ ì„¤ì • (ê¸°ì¡´ ë°ì´í„°ì— ìµœì í™”)
TRAINING_CONFIG = {
    'data_file': './gesture_data/merged_existing_data_16.npy',
    'metadata_file': './gesture_data/merged_data_metadata_16.json',
    'input_dim': 99,           # íŠ¹ì§• ì°¨ì› (84 + 15)
    'num_classes': 16,         # ì œìŠ¤ì²˜ í´ë˜ìŠ¤ ìˆ˜
    'hidden_sizes': [512, 256, 128, 64],  # ëŒ€ëŸ‰ ë°ì´í„°ì— ì í•©í•œ êµ¬ì¡°
    'dropout_rate': 0.4,       # ê³¼ì í•© ë°©ì§€
    'use_batch_norm': True,    # ë°°ì¹˜ ì •ê·œí™” ì‚¬ìš©
    'batch_size': 128,         # ëŒ€ëŸ‰ ë°ì´í„°ìš© ë°°ì¹˜ í¬ê¸°
    'epochs': 100,             # ì¶©ë¶„í•œ í•™ìŠµ
    'learning_rate': 0.001,    # í•™ìŠµë¥ 
    'weight_decay': 1e-4,      # ê°€ì¤‘ì¹˜ ê°ì‡ 
    'train_ratio': 0.7,        # í•™ìŠµ ë°ì´í„° ë¹„ìœ¨
    'val_ratio': 0.15,         # ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
    'test_ratio': 0.15,        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
    'early_stopping_patience': 15,  # ì¡°ê¸° ì¢…ë£Œ
    'save_best_only': True,    # ìµœê³  ì„±ëŠ¥ë§Œ ì €ì¥
    'class_balancing': True,   # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©
}

# =============================================================================
# ë°ì´í„°ì…‹ í´ë˜ìŠ¤
# =============================================================================

class ExistingGestureDataset(Dataset):
    """ê¸°ì¡´ ìˆ˜ì§‘ ë°ì´í„°ìš© ë°ì´í„°ì…‹"""
    
    def __init__(self, features, labels):
        self.features = torch.FloatTensor(features)
        self.labels = torch.LongTensor(labels)
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

# =============================================================================
# MLP ëª¨ë¸ (ê¸°ì¡´ ë°ì´í„°ìš© ìµœì í™”)
# =============================================================================

class ExistingDataMLP(nn.Module):
    """ê¸°ì¡´ ë°ì´í„°ìš© MLP ëª¨ë¸"""
    
    def __init__(self, input_dim=99, num_classes=16, hidden_sizes=[512, 256, 128, 64], 
                 dropout_rate=0.4, use_batch_norm=True):
        super(ExistingDataMLP, self).__init__()
        
        self.input_dim = input_dim
        self.num_classes = num_classes
        self.use_batch_norm = use_batch_norm
        
        # ì…ë ¥ì¸µ ì •ê·œí™”
        if use_batch_norm:
            self.input_norm = nn.BatchNorm1d(input_dim)
        
        # ë„¤íŠ¸ì›Œí¬ êµ¬ì„±
        layers = []
        prev_size = input_dim
        
        for i, hidden_size in enumerate(hidden_sizes):
            # Linear layer
            layers.append(nn.Linear(prev_size, hidden_size))
            
            # Batch Normalization
            if use_batch_norm:
                layers.append(nn.BatchNorm1d(hidden_size))
            
            # Activation
            layers.append(nn.ReLU(inplace=True))
            
            # Dropout
            layers.append(nn.Dropout(dropout_rate))
            
            prev_size = hidden_size
        
        # ì¶œë ¥ì¸µ
        layers.append(nn.Linear(prev_size, num_classes))
        
        self.network = nn.Sequential(*layers)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._initialize_weights()
    
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # ì…ë ¥ ì •ê·œí™”
        if hasattr(self, 'input_norm'):
            x = self.input_norm(x)
        
        return self.network(x)

# =============================================================================
# ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
# =============================================================================

def load_existing_data(config):
    """ê¸°ì¡´ ë³‘í•© ë°ì´í„° ë¡œë”©"""
    print("ğŸ“ ê¸°ì¡´ ë³‘í•© ë°ì´í„° ë¡œë”© ì¤‘...")
    
    data_file = config['data_file']
    metadata_file = config['metadata_file']
    
    # ë°ì´í„° íŒŒì¼ í™•ì¸
    if not os.path.exists(data_file):
        print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_file}")
        print("ë¨¼ì € merge_existing_data.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return None, None
    
    # ë©”íƒ€ë°ì´í„° ë¡œë”©
    if os.path.exists(metadata_file):
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        print(f"   ğŸ“‹ ë©”íƒ€ë°ì´í„° ë¡œë”© ì™„ë£Œ")
        print(f"   - ìƒì„± ë‚ ì§œ: {metadata.get('creation_date', 'Unknown')}")
        print(f"   - ì²˜ë¦¬ëœ íŒŒì¼: {metadata.get('total_files_processed', 0)}ê°œ")
    
    # ë°ì´í„° ë¡œë”©
    try:
        data = np.load(data_file)
        features = data[:, :-1].astype(np.float32)
        labels = data[:, -1].astype(int)
        
        print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ")
        print(f"   - ì´ ìƒ˜í”Œ: {len(features):,}")
        print(f"   - íŠ¹ì§• ì°¨ì›: {features.shape[1]}")
        print(f"   - ë¼ë²¨ ë²”ìœ„: {labels.min()} ~ {labels.max()}")
        print(f"   - ë°ì´í„° ë²”ìœ„: [{features.min():.3f}, {features.max():.3f}]")
        
        # ì œìŠ¤ì²˜ë³„ ë¶„í¬
        label_counts = Counter(labels)
        print(f"   - ì œìŠ¤ì²˜ë³„ ë¶„í¬:")
        for label in sorted(label_counts.keys()):
            count = label_counts[label]
            gesture_name = LABEL_TO_NAME.get(label, f'unknown_{label}')
            percentage = count / len(labels) * 100
            print(f"     {label:2d} ({gesture_name:12s}): {count:6,} ({percentage:5.1f}%)")
        
        return features, labels
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
        return None, None

def preprocess_data(features, labels, config):
    """ë°ì´í„° ì „ì²˜ë¦¬"""
    print("\nğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    
    # ìœ íš¨í•œ ë¼ë²¨ë§Œ í•„í„°ë§
    valid_mask = (labels >= 0) & (labels < config['num_classes'])
    features = features[valid_mask]
    labels = labels[valid_mask]
    
    print(f"   - ìœ íš¨í•œ ìƒ˜í”Œ: {len(features):,}")
    
    # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
    nan_mask = np.isnan(features).any(axis=1)
    inf_mask = np.isinf(features).any(axis=1)
    invalid_mask = nan_mask | inf_mask
    
    if invalid_mask.any():
        print(f"   - ì œê±°ëœ ë¬´íš¨ ìƒ˜í”Œ: {invalid_mask.sum():,}")
        features = features[~invalid_mask]
        labels = labels[~invalid_mask]
    
    # íŠ¹ì§• ì •ê·œí™”
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    
    print(f"   - ì •ê·œí™” ì „ ë²”ìœ„: [{features.min():.3f}, {features.max():.3f}]")
    print(f"   - ì •ê·œí™” í›„ ë²”ìœ„: [{features_scaled.min():.3f}, {features_scaled.max():.3f}]")
    print(f"   - í‰ê· : {features_scaled.mean():.6f}, í‘œì¤€í¸ì°¨: {features_scaled.std():.6f}")
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• ë°ì´í„° ëŒ€ì‘)
    class_weights = None
    if config['class_balancing']:
        label_counts = Counter(labels)
        total_samples = len(labels)
        n_classes = len(label_counts)
        
        # ê· í˜• ê°€ì¤‘ì¹˜ ê³„ì‚°
        weights = {}
        for label in range(config['num_classes']):
            count = label_counts.get(label, 1)  # 0ê°œì¸ ê²½ìš° 1ë¡œ ì„¤ì •
            weights[label] = total_samples / (n_classes * count)
        
        class_weights = torch.FloatTensor([weights[i] for i in range(config['num_classes'])])
        print(f"   - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©ë¨")
    
    return features_scaled, labels, scaler, class_weights

def create_data_loaders(features, labels, config):
    """ë°ì´í„°ë¡œë” ìƒì„± (ê³„ì¸µí™” ë¶„í• )"""
    print("\nğŸ“¦ ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...")
    
    # ê³„ì¸µí™” ë¶„í•  (í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€)
    sss_test = StratifiedShuffleSplit(n_splits=1, test_size=config['test_ratio'], random_state=42)
    train_val_idx, test_idx = next(sss_test.split(features, labels))
    
    X_train_val, X_test = features[train_val_idx], features[test_idx]
    y_train_val, y_test = labels[train_val_idx], labels[test_idx]
    
    # ê²€ì¦ ë°ì´í„° ë¶„í• 
    val_size_adjusted = config['val_ratio'] / (config['train_ratio'] + config['val_ratio'])
    sss_val = StratifiedShuffleSplit(n_splits=1, test_size=val_size_adjusted, random_state=42)
    train_idx, val_idx = next(sss_val.split(X_train_val, y_train_val))
    
    X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]
    y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = ExistingGestureDataset(X_train, y_train)
    val_dataset = ExistingGestureDataset(X_val, y_val)
    test_dataset = ExistingGestureDataset(X_test, y_test)
    
    # ë°ì´í„°ë¡œë” ìƒì„± (Windows í˜¸í™˜ì„±)
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config['batch_size'], 
        shuffle=True, 
        num_workers=0,
        pin_memory=False
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=config['batch_size'], 
        shuffle=False, 
        num_workers=0,
        pin_memory=False
    )
    test_loader = DataLoader(
        test_dataset, 
        batch_size=config['batch_size'], 
        shuffle=False, 
        num_workers=0,
        pin_memory=False
    )
    
    print(f"   - í•™ìŠµ: {len(train_dataset):,} ìƒ˜í”Œ")
    print(f"   - ê²€ì¦: {len(val_dataset):,} ìƒ˜í”Œ") 
    print(f"   - í…ŒìŠ¤íŠ¸: {len(test_dataset):,} ìƒ˜í”Œ")
    
    # ë¶„í•  í›„ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    train_label_counts = Counter(y_train)
    val_label_counts = Counter(y_val)
    test_label_counts = Counter(y_test)
    
    print(f"   - í´ë˜ìŠ¤ ë¶„í¬ (í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸):")
    for label in sorted(set(labels)):
        gesture_name = LABEL_TO_NAME.get(label, f'unknown_{label}')
        train_count = train_label_counts.get(label, 0)
        val_count = val_label_counts.get(label, 0)
        test_count = test_label_counts.get(label, 0)
        print(f"     {label:2d} ({gesture_name:10s}): {train_count:4,}/{val_count:3,}/{test_count:3,}")
    
    return train_loader, val_loader, test_loader

# =============================================================================
# í•™ìŠµ í•¨ìˆ˜ë“¤
# =============================================================================

def train_epoch(model, train_loader, criterion, optimizer, device):
    """í•œ ì—í¬í¬ í•™ìŠµ"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for data, targets in train_loader:
        data, targets = data.to(device), targets.to(device)
        
        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, targets)
        
        loss.backward()
        # ê·¸ë¼ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ì•ˆì •ì„± í–¥ìƒ)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
    
    avg_loss = total_loss / len(train_loader)
    accuracy = 100. * correct / total
    
    return avg_loss, accuracy

def validate_epoch(model, val_loader, criterion, device):
    """ê²€ì¦ ì—í¬í¬"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for data, targets in val_loader:
            data, targets = data.to(device), targets.to(device)
            outputs = model(data)
            loss = criterion(outputs, targets)
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
    
    avg_loss = total_loss / len(val_loader)
    accuracy = 100. * correct / total
    
    return avg_loss, accuracy, all_predictions, all_targets

def train_model(model, train_loader, val_loader, config, device, class_weights=None):
    """ëª¨ë¸ í•™ìŠµ ë©”ì¸ í•¨ìˆ˜"""
    print("\nğŸš€ ê¸°ì¡´ ë°ì´í„° MLP ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
    print("=" * 60)
    
    # ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €
    if class_weights is not None:
        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))
        print("âš–ï¸ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©ë¨")
    else:
        criterion = nn.CrossEntropyLoss()
    
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=config['learning_rate'],
        weight_decay=config['weight_decay']
    )
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=5, verbose=True
    )
    
    # í•™ìŠµ ê¸°ë¡
    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': []
    }
    best_val_acc = 0
    best_model_state = None
    patience_counter = 0
    
    print(f"ğŸ”§ ëª¨ë¸ ì •ë³´:")
    print(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   - í•™ìŠµ ìƒ˜í”Œ: {len(train_loader.dataset):,}")
    print(f"   - ê²€ì¦ ìƒ˜í”Œ: {len(val_loader.dataset):,}")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {config['batch_size']}")
    print(f"   - í•™ìŠµë¥ : {config['learning_rate']}")
    
    training_start_time = time.time()
    
    for epoch in range(config['epochs']):
        epoch_start = time.time()
        
        # í•™ìŠµ
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, device
        )
        
        # ê²€ì¦
        val_loss, val_acc, _, _ = validate_epoch(
            model, val_loader, criterion, device
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step(val_acc)
        
        epoch_time = time.time() - epoch_start
        
        # ê¸°ë¡ ì €ì¥
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_state = model.state_dict().copy()
            patience_counter = 0
            status = "ğŸ¯ NEW BEST!"
        else:
            patience_counter += 1
            status = f"({patience_counter}/{config['early_stopping_patience']})"
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥ (ë§¤ 5 ì—í¬í¬ë§ˆë‹¤)
        if epoch % 5 == 0 or epoch == config['epochs'] - 1 or val_acc > best_val_acc:
            print(f"Epoch {epoch+1:3d}/{config['epochs']} | "
                  f"Train: {train_loss:.4f}/{train_acc:.2f}% | "
                  f"Val: {val_loss:.4f}/{val_acc:.2f}% | "
                  f"Time: {epoch_time:.1f}s | "
                  f"LR: {optimizer.param_groups[0]['lr']:.6f} | "
                  f"{status}")
        
        # ì¡°ê¸° ì¢…ë£Œ
        if patience_counter >= config['early_stopping_patience']:
            print(f"â¹ï¸ ì¡°ê¸° ì¢…ë£Œ (patience={config['early_stopping_patience']})")
            break
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
    model.load_state_dict(best_model_state)
    
    total_training_time = time.time() - training_start_time
    
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"   - ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.2f}%")
    print(f"   - ì´ í•™ìŠµ ì‹œê°„: {total_training_time/60:.1f}ë¶„")
    print(f"   - ì—í¬í¬ë‹¹ í‰ê· : {total_training_time/(epoch+1):.1f}ì´ˆ")
    
    return model, history

def evaluate_model(model, test_loader, device):
    """ëª¨ë¸ í‰ê°€"""
    print("\nğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
    
    criterion = nn.CrossEntropyLoss()
    test_loss, test_acc, predictions, targets = validate_epoch(
        model, test_loader, criterion, device
    )
    
    print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   - ì†ì‹¤: {test_loss:.4f}")
    print(f"   - ì •í™•ë„: {test_acc:.2f}%")
    
    # ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ
    target_names = [LABEL_TO_NAME.get(i, f'class_{i}') for i in range(TRAINING_CONFIG['num_classes'])]
    print(f"\nğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ:")
    print(classification_report(
        targets, predictions, 
        target_names=target_names,
        zero_division=0
    ))
    
    return test_acc, predictions, targets

def plot_results(history, predictions, targets, save_path='existing_mlp_results.png'):
    """ê²°ê³¼ ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. í•™ìŠµ ê³¡ì„  (ì†ì‹¤)
    axes[0,0].plot(history['train_loss'], label='Train Loss', color='blue')
    axes[0,0].plot(history['val_loss'], label='Validation Loss', color='red')
    axes[0,0].set_title('Training and Validation Loss')
    axes[0,0].set_xlabel('Epoch')
    axes[0,0].set_ylabel('Loss')
    axes[0,0].legend()
    axes[0,0].grid(True)
    
    # 2. í•™ìŠµ ê³¡ì„  (ì •í™•ë„)
    axes[0,1].plot(history['train_acc'], label='Train Accuracy', color='blue')
    axes[0,1].plot(history['val_acc'], label='Validation Accuracy', color='red')
    axes[0,1].set_title('Training and Validation Accuracy')
    axes[0,1].set_xlabel('Epoch')
    axes[0,1].set_ylabel('Accuracy (%)')
    axes[0,1].legend()
    axes[0,1].grid(True)
    
    # 3. í˜¼ë™ í–‰ë ¬
    cm = confusion_matrix(targets, predictions)
    target_names = [LABEL_TO_NAME.get(i, f'class_{i}') for i in range(TRAINING_CONFIG['num_classes'])]
    
    sns.heatmap(
        cm, annot=True, fmt='d', cmap='Blues',
        xticklabels=target_names, yticklabels=target_names,
        ax=axes[1,0]
    )
    axes[1,0].set_title('Confusion Matrix')
    axes[1,0].set_xlabel('Predicted')
    axes[1,0].set_ylabel('True')
    
    # 4. í´ë˜ìŠ¤ë³„ ì •í™•ë„
    class_accuracies = []
    for i in range(TRAINING_CONFIG['num_classes']):
        mask = np.array(targets) == i
        if mask.sum() > 0:
            acc = accuracy_score(np.array(targets)[mask], np.array(predictions)[mask])
            class_accuracies.append(acc * 100)
        else:
            class_accuracies.append(0)
    
    bars = axes[1,1].bar(range(TRAINING_CONFIG['num_classes']), class_accuracies)
    axes[1,1].set_title('Class-wise Accuracy')
    axes[1,1].set_xlabel('Class')
    axes[1,1].set_ylabel('Accuracy (%)')
    axes[1,1].set_xticks(range(TRAINING_CONFIG['num_classes']))
    axes[1,1].set_xticklabels([LABEL_TO_NAME.get(i, f'{i}') for i in range(TRAINING_CONFIG['num_classes'])], 
                              rotation=45, ha='right')
    
    # ìƒ‰ìƒ ì½”ë”©
    for bar, acc in zip(bars, class_accuracies):
        if acc >= 95:
            bar.set_color('darkgreen')
        elif acc >= 90:
            bar.set_color('green')
        elif acc >= 80:
            bar.set_color('orange')
        else:
            bar.set_color('red')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"ğŸ“Š ê²°ê³¼ ê·¸ë˜í”„ê°€ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def save_model(model, scaler, config, test_accuracy):
    """ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸° ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ëª¨ë¸ ì €ì¥
    model_path = f'existing_mlp_model_{test_accuracy:.1f}pct.pth'
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': config,
        'gesture_labels': GESTURE_LABELS,
        'label_to_name': LABEL_TO_NAME,
        'model_class': 'ExistingDataMLP',
        'test_accuracy': test_accuracy,
        'timestamp': timestamp,
        'data_source': 'existing_collected_data'
    }, model_path)
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    scaler_path = f'existing_mlp_scaler.pkl'
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ:")
    print(f"   - ëª¨ë¸: {model_path}")
    print(f"   - ìŠ¤ì¼€ì¼ëŸ¬: {scaler_path}")
    print(f"   - í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.2f}%")

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¤– ê¸°ì¡´ ìˆ˜ì§‘ ë°ì´í„° MLP ëª¨ë¸ í•™ìŠµ")
    print("data_collect_improved.pyë¡œ ìˆ˜ì§‘ëœ ë°ì´í„° í™œìš©")
    print("=" * 60)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ë°ì´í„° ë¡œë”©
    features, labels = load_existing_data(TRAINING_CONFIG)
    if features is None:
        return
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    features_scaled, labels, scaler, class_weights = preprocess_data(
        features, labels, TRAINING_CONFIG
    )
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader, val_loader, test_loader = create_data_loaders(
        features_scaled, labels, TRAINING_CONFIG
    )
    
    # ëª¨ë¸ ìƒì„±
    print(f"\nğŸ§  ê¸°ì¡´ ë°ì´í„° MLP ëª¨ë¸ ìƒì„±...")
    model = ExistingDataMLP(
        input_dim=TRAINING_CONFIG['input_dim'],
        num_classes=TRAINING_CONFIG['num_classes'],
        hidden_sizes=TRAINING_CONFIG['hidden_sizes'],
        dropout_rate=TRAINING_CONFIG['dropout_rate'],
        use_batch_norm=TRAINING_CONFIG['use_batch_norm']
    ).to(device)
    
    print(f"   - ì…ë ¥ ì°¨ì›: {TRAINING_CONFIG['input_dim']}")
    print(f"   - ìˆ¨ì€ì¸µ: {TRAINING_CONFIG['hidden_sizes']}")
    print(f"   - ì¶œë ¥ í´ë˜ìŠ¤: {TRAINING_CONFIG['num_classes']}")
    print(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    
    # ëª¨ë¸ í•™ìŠµ
    trained_model, history = train_model(
        model, train_loader, val_loader, TRAINING_CONFIG, device, class_weights
    )
    
    # ëª¨ë¸ í‰ê°€
    test_accuracy, predictions, targets = evaluate_model(
        trained_model, test_loader, device
    )
    
    # ê²°ê³¼ ì‹œê°í™”
    plot_results(history, predictions, targets)
    
    # ëª¨ë¸ ì €ì¥
    save_model(trained_model, scaler, TRAINING_CONFIG, test_accuracy)
    
    print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    print(f"   - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.2f}%")
    print(f"   - ëª¨ë¸ íŒŒì¼: existing_mlp_model_{test_accuracy:.1f}pct.pth")
    print(f"   - ë‹¤ìŒ ë‹¨ê³„: ì‹¤ì‹œê°„ í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ ì‘ì„±")

if __name__ == "__main__":
    main()