# collect_sequence_data.py
"""
ì‹œí€€ìŠ¤ ì œìŠ¤ì²˜ ë°ì´í„° ìˆ˜ì§‘ (TCNìš©)
ì‹œê³„ë°©í–¥, ë°˜ì‹œê³„ë°©í–¥, ë³„ ëª¨ì–‘ ë“± ë™ì  ì œìŠ¤ì²˜ ìˆ˜ì§‘

Author: AIoT Project Team
Date: 2024
"""

import cv2
import mediapipe as mp
import numpy as np
import pandas as pd
import os
import time
import json
from datetime import datetime
from collections import deque
# import keyboard  # ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

# =============================================================================
# ì„¤ì • ë° ìƒìˆ˜
# =============================================================================

# ë™ì  ì œìŠ¤ì²˜ ì„¤ì • (ì‚¬ìš©ìê°€ ì§ì ‘ ì…ë ¥)
SEQUENCE_GESTURES = {}  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì‹œì‘
GESTURE_NAMES = []      # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì‹œì‘
LABEL_TO_NAME = {}      # ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì‹œì‘

# ë°ì´í„° ìˆ˜ì§‘ ì„¤ì •
COLLECTION_CONFIG = {
    'sequence_length': 60,           # ì‹œí€€ìŠ¤ ê¸¸ì´ (í”„ë ˆì„ ìˆ˜)
    'fps': 30,                       # ëª©í‘œ FPS
    'samples_per_gesture': 200,      # ì œìŠ¤ì²˜ë‹¹ ìƒ˜í”Œ ìˆ˜
    'min_confidence': 0.7,           # ìµœì†Œ ê°ì§€ ì‹ ë¢°ë„
    'gesture_duration': 2.5,         # ì œìŠ¤ì²˜ ìˆ˜í–‰ ì‹œê°„ (ì´ˆ)
    'rest_duration': 1.0,            # ì œìŠ¤ì²˜ ê°„ íœ´ì‹ ì‹œê°„ (ì´ˆ)
    'auto_save_interval': 20,        # ìë™ ì €ì¥ ê°„ê²©
    'data_dir': './gesture_data/sequence_data',
    'show_guidelines': True,         # ê°€ì´ë“œë¼ì¸ í‘œì‹œ
    'quality_check': True,           # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
}

# MediaPipe ì„¤ì •
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

# ìƒ‰ìƒ ì„¤ì • (BGR)
COLORS = {
    'good': (0, 255, 0),      # ì´ˆë¡ìƒ‰ (ìˆ˜ì§‘ ì¤‘)
    'waiting': (0, 255, 255),  # ë…¸ë€ìƒ‰ (ëŒ€ê¸°)
    'rest': (255, 0, 0),      # íŒŒë€ìƒ‰ (íœ´ì‹)
    'complete': (255, 0, 255), # ë§ˆì  íƒ€ (ì™„ë£Œ)
    'text': (255, 255, 255),   # í°ìƒ‰
    'bg': (50, 50, 50),        # íšŒìƒ‰
    'trail': (0, 165, 255),    # ì£¼í™©ìƒ‰ (ê¶¤ì )
}

# =============================================================================
# ë°ì´í„° ìˆ˜ì§‘ í´ë˜ìŠ¤
# =============================================================================

class SequenceGestureCollector:
    """ì‹œí€€ìŠ¤ ì œìŠ¤ì²˜ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, config):
        self.config = config
        self.current_gesture_name = None
        self.current_sample = 0
        self.collected_data = []
        self.gesture_names = []  # ìˆ˜ì§‘ëœ ì œìŠ¤ì²˜ ì´ë¦„ë“¤
        
        # ìˆ˜ì§‘ ìƒíƒœ
        self.collecting = False
        self.in_rest = False
        self.sequence_buffer = deque(maxlen=config['sequence_length'])
        self.trail_points = deque(maxlen=30)  # ê¶¤ì  í‘œì‹œìš©
        
        # íƒ€ì´ë°
        self.gesture_start_time = 0
        self.rest_start_time = 0
        
        # MediaPipe ì´ˆê¸°í™”
        self.hands = mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=1,
            min_detection_confidence=config['min_confidence'],
            min_tracking_confidence=0.5
        )
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(config['data_dir'], exist_ok=True)
        
        # ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
        self.metadata = {
            'collection_start': datetime.now().isoformat(),
            'config': config,
            'gestures': {},  # ë™ì ìœ¼ë¡œ ì±„ì›Œì§ˆ ì˜ˆì •
            'collected_samples': {}  # ë™ì ìœ¼ë¡œ ì±„ì›Œì§ˆ ì˜ˆì •
        }
        
        print("ğŸ¯ ì‹œí€€ìŠ¤ ì œìŠ¤ì²˜ ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ!")
        print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {config['data_dir']}")
        print(f"ğŸ’¡ ìƒˆë¡œìš´ ì œìŠ¤ì²˜ ì´ë¦„ì„ ì…ë ¥í•˜ì—¬ ìˆ˜ì§‘ì„ ì‹œì‘í•˜ì„¸ìš”!")
    
    def extract_hand_landmarks(self, image):
        """ì† ëœë“œë§ˆí¬ ì¶”ì¶œ"""
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.hands.process(image_rgb)
        
        landmarks = []
        handedness = None
        confidence = 0.0
        
        if results.multi_hand_landmarks and results.multi_handedness:
            hand_landmarks = results.multi_hand_landmarks[0]
            hand_info = results.multi_handedness[0]
            
            handedness = hand_info.classification[0].label
            confidence = hand_info.classification[0].score
            
            # ëœë“œë§ˆí¬ ì¶”ì¶œ
            for lm in hand_landmarks.landmark:
                landmarks.extend([lm.x, lm.y, lm.z])
            
            # ê¶¤ì  í¬ì¸íŠ¸ ì¶”ê°€ (ê²€ì§€ ë)
            if len(landmarks) >= 24:  # 8ë²ˆ ëœë“œë§ˆí¬ (ê²€ì§€ ë)
                finger_tip = (landmarks[24], landmarks[25])  # x, y
                self.trail_points.append(finger_tip)
        
        return landmarks, handedness, confidence
    
    def create_features_from_landmarks(self, landmarks):
        """ëœë“œë§ˆí¬ì—ì„œ íŠ¹ì§• ë²¡í„° ìƒì„± (ê¸°ì¡´ ë°©ì‹ê³¼ ë™ì¼)"""
        if len(landmarks) < 63:  # 21 * 3
            return None
        
        try:
            # 21ê°œ ê´€ì ˆ ì¢Œí‘œ ì¬êµ¬ì„±
            joint = np.array(landmarks).reshape(21, 3)
            
            # ê¸°ë³¸ ì¢Œí‘œì— visibility ì¶”ê°€ (1.0ìœ¼ë¡œ ì„¤ì •)
            joint_with_vis = np.column_stack([joint, np.ones(21)])
            
            # ë²¡í„° ê³„ì‚° (ê¸°ì¡´ ë°©ì‹ê³¼ ë™ì¼)
            v1 = joint_with_vis[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3]
            v2 = joint_with_vis[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3]
            v = v2 - v1
            
            # ì •ê·œí™”
            norms = np.linalg.norm(v, axis=1)
            norms[norms == 0] = 1e-6
            v = v / norms[:, np.newaxis]
            
            # ê´€ì ˆ ê°„ ê°ë„ ê³„ì‚°
            angle = np.arccos(np.clip(np.einsum('nt,nt->n',
                v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:],
                v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:]), -1, 1))
            
            angle = np.degrees(angle)
            
            # íŠ¹ì§• ë²¡í„°: ê´€ì ˆ(84) + ê°ë„(15) = 99ì°¨ì›
            features = np.concatenate([joint_with_vis.flatten(), angle])
            
            # ìœ íš¨ì„± ê²€ì‚¬
            if np.isnan(features).any() or np.isinf(features).any():
                return None
            
            return features
            
        except Exception as e:
            print(f"íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def update_collection_state(self):
        """ìˆ˜ì§‘ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        current_time = time.time()
        
        if self.collecting:
            # ì œìŠ¤ì²˜ ìˆ˜ì§‘ ì¤‘
            elapsed = current_time - self.gesture_start_time
            if elapsed >= self.config['gesture_duration']:
                # ì œìŠ¤ì²˜ ì™„ë£Œ
                self.complete_current_gesture()
                self.start_rest_period()
        
        elif self.in_rest:
            # íœ´ì‹ ì¤‘
            elapsed = current_time - self.rest_start_time
            if elapsed >= self.config['rest_duration']:
                # íœ´ì‹ ì™„ë£Œ
                self.in_rest = False
                print(f"\nâœ… íœ´ì‹ ì™„ë£Œ! ë‹¤ìŒ ìƒ˜í”Œ ì¤€ë¹„")
    
    def set_current_gesture(self, gesture_name):
        """í˜„ì¬ ìˆ˜ì§‘í•  ì œìŠ¤ì²˜ ì„¤ì •"""
        self.current_gesture_name = gesture_name
        
        # ìƒˆë¡œìš´ ì œìŠ¤ì²˜ì¸ ê²½ìš° ì¶”ê°€
        if gesture_name not in self.gesture_names:
            self.gesture_names.append(gesture_name)
            self.metadata['collected_samples'][gesture_name] = 0
            print(f"âœ… ìƒˆë¡œìš´ ì œìŠ¤ì²˜ '{gesture_name}' ì¶”ê°€ë¨")
        
        # í•´ë‹¹ ì œìŠ¤ì²˜ì˜ í˜„ì¬ ìƒ˜í”Œ ìˆ˜ í™•ì¸
        self.current_sample = self.metadata['collected_samples'][gesture_name]
        print(f"ğŸ“Š '{gesture_name}' í˜„ì¬ ìƒ˜í”Œ ìˆ˜: {self.current_sample}")
    
    def start_collecting(self):
        """ì œìŠ¤ì²˜ ìˆ˜ì§‘ ì‹œì‘"""
        if self.current_gesture_name is None:
            print("âŒ ì œìŠ¤ì²˜ ì´ë¦„ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ì œìŠ¤ì²˜ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return False
            
        if not self.collecting and not self.in_rest:
            self.collecting = True
            self.gesture_start_time = time.time()
            self.sequence_buffer.clear()
            self.trail_points.clear()
            
            sample_num = self.current_sample + 1
            total_samples = self.config['samples_per_gesture']
            
            print(f"\nğŸ¯ ìˆ˜ì§‘ ì‹œì‘: {self.current_gesture_name} ({sample_num}/{total_samples})")
            print(f"â±ï¸ {self.config['gesture_duration']}ì´ˆ ë™ì•ˆ ì œìŠ¤ì²˜ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”!")
            return True
        
        return False
    
    def complete_current_gesture(self):
        """í˜„ì¬ ì œìŠ¤ì²˜ ìˆ˜ì§‘ ì™„ë£Œ"""
        if len(self.sequence_buffer) >= self.config['sequence_length']:
            # ì‹œí€€ìŠ¤ ë°ì´í„° ì €ì¥ (ë¼ë²¨ì€ -1ë¡œ ì„¤ì •, ë‚˜ì¤‘ì— í• ë‹¹)
            sequence_data = {
                'gesture': self.current_gesture_name,
                'label': -1,  # ë‚˜ì¤‘ì— í• ë‹¹
                'sample_id': self.current_sample,
                'timestamp': datetime.now().isoformat(),
                'sequence': [feature.tolist() if hasattr(feature, 'tolist') else feature 
                           for feature in self.sequence_buffer],  # numpy array â†’ list ë³€í™˜
                'sequence_length': len(self.sequence_buffer)
            }
            
            self.collected_data.append(sequence_data)
            self.metadata['collected_samples'][self.current_gesture_name] += 1
            
            print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {self.current_gesture_name} ìƒ˜í”Œ {self.current_sample + 1}")
            
            # ë‹¤ìŒ ìƒ˜í”Œë¡œ ì´ë™
            self.current_sample += 1
            
            # ìë™ ì €ì¥
            if self.current_sample % self.config['auto_save_interval'] == 0:
                self.save_data()
            
            # ì œìŠ¤ì²˜ ì™„ë£Œ ì²´í¬
            if self.current_sample >= self.config['samples_per_gesture']:
                print(f"\nğŸ‰ '{self.current_gesture_name}' ìˆ˜ì§‘ ì™„ë£Œ! ({self.config['samples_per_gesture']}ê°œ)")
                print("ìƒˆë¡œìš´ ì œìŠ¤ì²˜ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ 'Q'ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œí•˜ì„¸ìš”.")
                self.current_gesture_name = None  # ì œìŠ¤ì²˜ ì´ˆê¸°í™”
        
        self.collecting = False
    
    def start_rest_period(self):
        """íœ´ì‹ ê¸°ê°„ ì‹œì‘"""
        self.in_rest = True
        self.rest_start_time = time.time()
        print(f"ğŸ˜´ íœ´ì‹ ì‹œê°„: {self.config['rest_duration']}ì´ˆ")
    
    def get_gesture_input(self):
        """ì‚¬ìš©ìë¡œë¶€í„° ì œìŠ¤ì²˜ ì´ë¦„ ì…ë ¥ë°›ê¸°"""
        while True:
            try:
                gesture_name = input("\nğŸ¯ ìˆ˜ì§‘í•  ì œìŠ¤ì²˜ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit' ë˜ëŠ” 'q'): ").strip()
                
                if gesture_name.lower() in ['quit', 'q', 'exit']:
                    return None
                
                if len(gesture_name) == 0:
                    print("âŒ ì œìŠ¤ì²˜ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜
                clean_name = gesture_name.replace(' ', '_').replace('-', '_')
                clean_name = ''.join(c for c in clean_name if c.isalnum() or c == '_')
                
                if len(clean_name) == 0:
                    print("âŒ ìœ íš¨í•œ ì œìŠ¤ì²˜ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš” (ì•ŒíŒŒë²³, ìˆ«ì, ì–¸ë”ìŠ¤ì½”ì–´ë§Œ ì‚¬ìš©).")
                    continue
                
                return clean_name
                
            except KeyboardInterrupt:
                return None
            except Exception as e:
                print(f"âŒ ì…ë ¥ ì˜¤ë¥˜: {e}")
                continue
    
    def save_data(self):
        """ì¤‘ê°„ ì €ì¥"""
        if not self.collected_data:
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"sequence_data_temp_{timestamp}.json"
        filepath = os.path.join(self.config['data_dir'], filename)
        
        save_data = {
            'metadata': self.metadata,
            'data': self.collected_data
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥: {filename}")
    
    def save_final_data(self):
        """ìµœì¢… ë°ì´í„° ì €ì¥"""
        if not self.collected_data:
            print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # JSON í˜•íƒœë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_filename = f"sequence_gestures_{timestamp}.json"
        json_filepath = os.path.join(self.config['data_dir'], json_filename)
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        self.metadata['collection_end'] = datetime.now().isoformat()
        self.metadata['total_samples'] = len(self.collected_data)
        
        save_data = {
            'metadata': self.metadata,
            'data': self.collected_data
        }
        
        with open(json_filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        # NumPy í˜•íƒœë¡œë„ ì €ì¥ (í•™ìŠµìš©)
        self.save_numpy_format(timestamp)
        
        print(f"\nğŸ‰ ìµœì¢… ì €ì¥ ì™„ë£Œ!")
        print(f"   ğŸ“ JSON: {json_filename}")
        print(f"   ğŸ“ NumPy: sequence_gestures_{timestamp}.npz")
        print(f"   ğŸ“Š ì´ ìƒ˜í”Œ: {len(self.collected_data)}ê°œ")
        
        # í†µê³„ ì¶œë ¥
        self.print_collection_stats()
    
    def save_numpy_format(self, timestamp):
        """NumPy í˜•íƒœë¡œ ì €ì¥ (TCN í•™ìŠµìš©) - ë¼ë²¨ì€ ë‚˜ì¤‘ì— í• ë‹¹"""
        sequences = []
        gesture_names = []
        
        # ì œìŠ¤ì²˜ ì´ë¦„ì„ ë¼ë²¨ë¡œ ë§¤í•‘í•  ë”•ì…”ë„ˆë¦¬ ìƒì„±
        unique_gestures = list(set([data['gesture'] for data in self.collected_data]))
        gesture_to_label = {gesture: idx for idx, gesture in enumerate(unique_gestures)}
        
        for data in self.collected_data:
            # ì´ë¯¸ listë¡œ ë³€í™˜ëœ sequenceë¥¼ numpy arrayë¡œ ë³€í™˜
            sequence_list = data['sequence']
            if isinstance(sequence_list[0], list):
                # ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° numpy arrayë¡œ ë³€í™˜
                sequence = np.array(sequence_list)
            else:
                # ì´ë¯¸ numpy arrayì´ê±°ë‚˜ ë‹¨ìˆœ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                sequence = np.array(sequence_list)
            
            gesture_name = data['gesture']
            
            # ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶”ê¸°
            if len(sequence) == self.config['sequence_length']:
                sequences.append(sequence)
                gesture_names.append(gesture_name)
        
        if sequences:
            sequences = np.array(sequences)  # (N, seq_len, features)
            
            numpy_filename = f"sequence_gestures_{timestamp}.npz"
            numpy_filepath = os.path.join(self.config['data_dir'], numpy_filename)
            
            np.savez_compressed(
                numpy_filepath,
                sequences=sequences,
                gesture_names=np.array(gesture_names),  # ê° ìƒ˜í”Œì˜ ì œìŠ¤ì²˜ ì´ë¦„
                unique_gestures=unique_gestures,        # ìœ ë‹ˆí¬í•œ ì œìŠ¤ì²˜ ëª©ë¡
                gesture_to_label=gesture_to_label,      # ì œìŠ¤ì²˜ -> ë¼ë²¨ ë§¤í•‘
                config=self.config
            )
            
            print(f"   ğŸ“‹ ìˆ˜ì§‘ëœ ì œìŠ¤ì²˜: {unique_gestures}")
            print(f"   ğŸ·ï¸ ë¼ë²¨ ë§¤í•‘: {gesture_to_label}")
    
    def print_collection_stats(self):
        """ìˆ˜ì§‘ í†µê³„ ì¶œë ¥"""
        print(f"\nğŸ“Š ìˆ˜ì§‘ í†µê³„:")
        print("-" * 40)
        
        total_samples = 0
        for gesture_name in self.gesture_names:
            count = self.metadata['collected_samples'][gesture_name]
            total_samples += count
            percentage = (count / self.config['samples_per_gesture']) * 100
            print(f"   {gesture_name:15s}: {count:3d} ({percentage:5.1f}%)")
        
        print("-" * 40)
        print(f"   ì´ ìƒ˜í”Œ: {total_samples}")
        print(f"   ìˆ˜ì§‘ëœ ì œìŠ¤ì²˜ ìˆ˜: {len(self.gesture_names)}")
        
        if total_samples > 0:
            avg_sequence_len = np.mean([len(data['sequence']) for data in self.collected_data])
            print(f"   í‰ê·  ì‹œí€€ìŠ¤ ê¸¸ì´: {avg_sequence_len:.1f}")
    
    def draw_ui(self, image):
        """UI ê·¸ë¦¬ê¸°"""
        h, w = image.shape[:2]
        
        # í˜„ì¬ ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ
        if self.collecting:
            status_color = COLORS['good']
            status_text = "COLLECTING"
        elif self.in_rest:
            status_color = COLORS['rest']
            status_text = "RESTING"
        else:
            status_color = COLORS['waiting']
            status_text = "READY"
        
        # ë°°ê²½ ë°•ìŠ¤
        overlay = image.copy()
        cv2.rectangle(overlay, (10, 10), (w-10, 120), COLORS['bg'], -1)
        cv2.addWeighted(overlay, 0.7, image, 0.3, 0, image)
        
        # ì œìŠ¤ì²˜ ì •ë³´
        if self.current_gesture_name:
            gesture_name = self.current_gesture_name
            progress = f"{self.current_sample}/{self.config['samples_per_gesture']}"
            
            cv2.putText(image, f"Gesture: {gesture_name.upper()}", 
                       (20, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.7, COLORS['text'], 2)
            cv2.putText(image, f"Progress: {progress}", 
                       (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, COLORS['text'], 1)
        else:
            cv2.putText(image, "Gesture: NOT SET", 
                       (20, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.7, COLORS['no_hand'], 2)
            cv2.putText(image, "Enter gesture name first", 
                       (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, COLORS['text'], 1)
        
        cv2.putText(image, f"Status: {status_text}", 
                   (20, 85), cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 2)
        
        # íƒ€ì´ë¨¸
        if self.collecting:
            elapsed = time.time() - self.gesture_start_time
            remaining = max(0, self.config['gesture_duration'] - elapsed)
            cv2.putText(image, f"Time: {remaining:.1f}s", 
                       (w-150, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.7, status_color, 2)
        elif self.in_rest:
            elapsed = time.time() - self.rest_start_time
            remaining = max(0, self.config['rest_duration'] - elapsed)
            cv2.putText(image, f"Rest: {remaining:.1f}s", 
                       (w-150, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.7, status_color, 2)
        
        # ê¶¤ì  ê·¸ë¦¬ê¸°
        if len(self.trail_points) > 1:
            points = [(int(x * w), int(y * h)) for x, y in self.trail_points]
            for i in range(1, len(points)):
                cv2.line(image, points[i-1], points[i], COLORS['trail'], 3)
        
        # ê°€ì´ë“œë¼ì¸
        if self.config['show_guidelines'] and self.current_gesture_name:
            self.draw_guidelines(image, self.current_gesture_name)
        
        # ì œì–´ ê°€ì´ë“œ
        cv2.putText(image, "Controls: SPACE-Start, N-New Gesture, Q-Quit", 
                   (20, h-20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS['text'], 1)
        
        return image
    
    def draw_guidelines(self, image, gesture_name):
        """ì œìŠ¤ì²˜ë³„ ê°€ì´ë“œë¼ì¸ ê·¸ë¦¬ê¸°"""
        h, w = image.shape[:2]
        center_x, center_y = w // 2, h // 2
        radius = min(w, h) // 4
        
        # ë°˜íˆ¬ëª… ì˜¤ë²„ë ˆì´
        overlay = image.copy()
        
        if gesture_name in ['clockwise', 'counter_clockwise']:
            # ì›í˜• ê°€ì´ë“œ
            cv2.circle(overlay, (center_x, center_y), radius, COLORS['trail'], 2)
            # ë°©í–¥ í™”ì‚´í‘œ
            if gesture_name == 'clockwise':
                cv2.arrowedLine(overlay, (center_x + radius//2, center_y - radius//2),
                              (center_x + radius//2, center_y + radius//2), COLORS['trail'], 3)
            else:
                cv2.arrowedLine(overlay, (center_x + radius//2, center_y + radius//2),
                              (center_x + radius//2, center_y - radius//2), COLORS['trail'], 3)
        
        elif gesture_name == 'star':
            # ë³„ ëª¨ì–‘ ê°€ì´ë“œ
            points = []
            for i in range(10):
                angle = i * np.pi / 5
                if i % 2 == 0:
                    r = radius
                else:
                    r = radius // 2
                x = int(center_x + r * np.cos(angle - np.pi/2))
                y = int(center_y + r * np.sin(angle - np.pi/2))
                points.append((x, y))
            
            for i in range(len(points)):
                cv2.line(overlay, points[i], points[(i+1) % len(points)], COLORS['trail'], 2)
        
        elif gesture_name == 'triangle':
            # ì‚¼ê°í˜• ê°€ì´ë“œ
            points = [
                (center_x, center_y - radius),
                (center_x - radius//2, center_y + radius//2),
                (center_x + radius//2, center_y + radius//2),
                (center_x, center_y - radius)
            ]
            for i in range(len(points)-1):
                cv2.line(overlay, points[i], points[i+1], COLORS['trail'], 2)
        
        elif gesture_name == 'square':
            # ì‚¬ê°í˜• ê°€ì´ë“œ
            top_left = (center_x - radius//2, center_y - radius//2)
            bottom_right = (center_x + radius//2, center_y + radius//2)
            cv2.rectangle(overlay, top_left, bottom_right, COLORS['trail'], 2)
        
        # ì˜¤ë²„ë ˆì´ ì ìš©
        cv2.addWeighted(overlay, 0.3, image, 0.7, 0, image)

# =============================================================================
# ë©”ì¸ ìˆ˜ì§‘ í•¨ìˆ˜
# =============================================================================

def main():
    """ë©”ì¸ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜"""
    print("ğŸ¯ ì‹œí€€ìŠ¤ ì œìŠ¤ì²˜ ë°ì´í„° ìˆ˜ì§‘ (TCNìš©)")
    print("=" * 60)
    
    # ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = SequenceGestureCollector(COLLECTION_CONFIG)
    
    # ì›¹ìº  ì´ˆê¸°í™”
    print("ğŸ“¹ ì›¹ìº  ì´ˆê¸°í™” ì¤‘...")
    cap = cv2.VideoCapture(0)
    
    if not cap.isOpened():
        print("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í•´ìƒë„ ë° FPS ì„¤ì •
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    cap.set(cv2.CAP_PROP_FPS, COLLECTION_CONFIG['fps'])
    
    print("âœ… ì´ˆê¸°í™” ì™„ë£Œ!")
    print("\nğŸ¯ ìˆ˜ì§‘ ì‹œì‘!")
    print("ğŸ”§ ì œì–´:")
    print("   N - ìƒˆë¡œìš´ ì œìŠ¤ì²˜ ì…ë ¥")
    print("   SPACE - ì œìŠ¤ì²˜ ìˆ˜ì§‘ ì‹œì‘")
    print("   Q - ì¢…ë£Œ")
    print("=" * 60)
    
    # ì²« ë²ˆì§¸ ì œìŠ¤ì²˜ ì…ë ¥ ë°›ê¸°
    print("\nğŸ’¡ ë¨¼ì € ìˆ˜ì§‘í•  ì œìŠ¤ì²˜ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
    gesture_name = collector.get_gesture_input()
    
    if gesture_name is None:
        print("ğŸ‘‹ ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        cap.release()
        cv2.destroyAllWindows()
        collector.hands.close()
        return
    
    collector.set_current_gesture(gesture_name)
    print("Space í‚¤ë¥¼ ëˆŒëŸ¬ì„œ ìˆ˜ì§‘ì„ ì‹œì‘í•˜ì„¸ìš”!")
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("âŒ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            frame = cv2.flip(frame, 1)  # ì¢Œìš° ë°˜ì „
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            collector.update_collection_state()
            
            # ì† ëœë“œë§ˆí¬ ì¶”ì¶œ
            landmarks, handedness, confidence = collector.extract_hand_landmarks(frame)
            
            # ìˆ˜ì§‘ ì¤‘ì¸ ê²½ìš° ë°ì´í„° ì¶”ê°€
            if collector.collecting and len(landmarks) > 0 and confidence >= COLLECTION_CONFIG['min_confidence']:
                features = collector.create_features_from_landmarks(landmarks)
                if features is not None:
                    collector.sequence_buffer.append(features)
            
            # ì† ê·¸ë¦¬ê¸°
            if len(landmarks) > 0:
                # ëœë“œë§ˆí¬ë¥¼ ë‹¤ì‹œ MediaPipe í˜•íƒœë¡œ ë³€í™˜í•´ì„œ ê·¸ë¦¬ê¸°
                # ê°„ë‹¨í•˜ê²Œ ê²€ì§€ ëì ë§Œ í‘œì‹œ
                h, w = frame.shape[:2]
                if len(landmarks) >= 24:
                    finger_x = int(landmarks[24] * w)
                    finger_y = int(landmarks[25] * h)
                    cv2.circle(frame, (finger_x, finger_y), 8, COLORS['good'], -1)
            
            # UI ê·¸ë¦¬ê¸°
            frame = collector.draw_ui(frame)
            
            # í™”ë©´ í‘œì‹œ
            cv2.imshow('Sequence Gesture Collection', frame)
            
            # í‚¤ ì…ë ¥ ì²˜ë¦¬
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                print("\nğŸ‘‹ ì‚¬ìš©ìê°€ ì¢…ë£Œë¥¼ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.")
                break
            elif key == ord(' '):
                collector.start_collecting()
            elif key == ord('n'):
                print("\nğŸ”„ ìƒˆë¡œìš´ ì œìŠ¤ì²˜ ì…ë ¥...")
                new_gesture = collector.get_gesture_input()
                if new_gesture is None:
                    print("âŒ ì œìŠ¤ì²˜ ì…ë ¥ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    collector.set_current_gesture(new_gesture)
                    print("Space í‚¤ë¥¼ ëˆŒëŸ¬ì„œ ìˆ˜ì§‘ì„ ì‹œì‘í•˜ì„¸ìš”!")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì¸í„°ëŸ½íŠ¸ë¡œ ì¢…ë£Œë©ë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ì •ë¦¬
        cap.release()
        cv2.destroyAllWindows()
        collector.hands.close()
        
        # ìµœì¢… ì €ì¥
        if collector.collected_data:
            collector.save_final_data()
        
        print("\nğŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")

if __name__ == "__main__":
    main()