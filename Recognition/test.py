"""
í†µí•© ì œìŠ¤ì²˜ + ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ

"""
import warnings

# Google Protobuf ê´€ë ¨ deprecated ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", message="SymbolDatabase.GetPrototype() is deprecated")
warnings.filterwarnings("ignore", message=".*GetPrototype.*is deprecated.*")
warnings.filterwarnings("ignore", category=UserWarning, module="google.protobuf")

import cv2
import mediapipe as mp
import numpy as np
import torch
import torch.nn as nn
import pickle
import time
import glob
from collections import deque, Counter
import os

import speech_recognition as sr
import requests
import io
import wave
import Levenshtein
import threading


# ================== ì„œë²„ ì„¤ì • ==================
VOICE_SERVER_URL = 'http://192.168.219.108:5000/voice'  # ìŒì„± ì„œë²„ URL
GESTURE_SERVER_URL = 'http://192.168.219.108:5000/gesture'  # ì œìŠ¤ì²˜ ì„œë²„ URL
USER_UID = "ot2SrPF7bcdGBpm2ACDyVDwkpPF2"  # ì‚¬ìš©ì ê³ ìœ  ID (Firebase UID)

# ================== ì„œë²„ ì „ì†¡ í•¨ìˆ˜ ==================

# ì „ì†¡ ê°„ê²© ì œì–´ ë³€ìˆ˜ë“¤
last_voice_command = None
last_voice_time = 0
last_gesture_command = None
last_gesture_time = 0
SEND_COOLDOWN = 2.0 

# ì „ì†¡ ê°„ê²© ì œì–´
last_sent_gesture = None
last_sent_gesture_time = 0
last_sent_voice = None
last_sent_voice_time = 0
gesture_delay = 2.0     # ë‹¤ë¥¸ ì œìŠ¤ì²˜ ì „ì†¡ ìµœì†Œ ì§€ì—°
gesture_resend = 5.0    # ê°™ì€ ì œìŠ¤ì²˜ ì¬ì „ì†¡ ê°„ê²©
voice_delay = 2.0
voice_resend = 5.0


def send_to_server(url, key, value, command_type=None):
    """ì„œë²„ ì „ì†¡ (UID í¬í•¨, ì‘ë‹µ ìƒì„¸ í™•ì¸)"""
    data = {
        "uid": USER_UID,
        key: value
    }
    if command_type:  # ìŒì„±/ì œìŠ¤ì²˜ íƒ€ì… êµ¬ë¶„
        data["type"] = command_type

    try:
        print(f"\n [ì„œë²„ ì „ì†¡] {key}: {value} (íƒ€ì…: {command_type})")
        print(f" ì‚¬ìš©ì UID: {USER_UID}")
        print(f" ì „ì†¡ URL: {url}")
        print(f" ì „ì†¡ ë°ì´í„°: {data}")
        
        response = requests.post(url, json=data, timeout=10)
        
        # ì‘ë‹µ ìƒì„¸ ì •ë³´ ì¶œë ¥
        print(f"\n  [ì„œë²„ ì‘ë‹µ]")
        print(f"   ìƒíƒœ ì½”ë“œ: {response.status_code}")
        print(f"   ì‘ë‹µ ì‹œê°„: {response.elapsed.total_seconds():.3f}ì´ˆ")
        print(f"   ì‘ë‹µ í—¤ë”: {dict(response.headers)}")
        print(f"   ì‘ë‹µ ë³¸ë¬¸: '{response.text}'")
        print(f"   ì‘ë‹µ ê¸¸ì´: {len(response.text)} ë¬¸ì")
        
        if response.status_code == 200:
            print(f"  ì „ì†¡ ì„±ê³µ! ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤.")
            if response.text:
                print(f"  ì„œë²„ ë©”ì‹œì§€: {response.text}")
            else:
                print(f"  ì„œë²„ì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤.")
        elif response.status_code == 404:
            print(f"  404 ì˜¤ë¥˜: URLì´ ì˜ëª»ë˜ì—ˆê±°ë‚˜ ì„œë²„ì— í•´ë‹¹ ì—”ë“œí¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print(f"  í™•ì¸ì‚¬í•­: {url} ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
        elif response.status_code == 500:
            print(f"  500 ì˜¤ë¥˜: ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            print(f"  ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")
        else:
            print(f"  ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ ì½”ë“œ: {response.status_code}")
            
    except requests.exceptions.ConnectionError as e:
        print(f"  [ì—°ê²° ì˜¤ë¥˜] ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ì˜¤ë¥˜ ë‚´ìš©: {e}")
    except requests.exceptions.Timeout as e:
        print(f"  [íƒ€ì„ì•„ì›ƒ] ì„œë²„ ì‘ë‹µì´ 10ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
        print(f"   ì˜¤ë¥˜ ë‚´ìš©: {e}")
    except Exception as e:
        print(f"  [ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜] {type(e).__name__}: {e}")
        
    print("="*50)


# ==================== ì œìŠ¤ì²˜ ì „ì†¡ ====================
def try_send_gesture(gesture):
    global last_sent_gesture, last_sent_gesture_time
    now = time.time()

    if gesture != last_sent_gesture:
        if now - last_sent_gesture_time >= gesture_delay:
            threading.Thread(
                target=send_to_server,
                args=(GESTURE_SERVER_URL, "gesture", gesture),  # ì œìŠ¤ì²˜ëŠ” íƒ€ì… í•„ìš” ì—†ìŒ
                daemon=True
            ).start()
            last_sent_gesture = gesture
            last_sent_gesture_time = now
            print(f"[ì œìŠ¤ì²˜ ì „ì†¡] sent: {gesture}")

    elif now - last_sent_gesture_time >= gesture_resend:
        threading.Thread(
            target=send_to_server,
            args=(GESTURE_SERVER_URL, "gesture", gesture),
            daemon=True
        ).start()
        last_sent_gesture_time = now
        print(f"[ì œìŠ¤ì²˜ ì „ì†¡] re-sent: {gesture}")


# ==================== ìŒì„± ì „ì†¡ ====================
def try_send_voice(voice_command):
    global last_sent_voice, last_sent_voice_time
    now = time.time()

    if voice_command != last_sent_voice:
        if now - last_sent_voice_time >= voice_delay:
            threading.Thread(
                target=send_to_server,
                args=(VOICE_SERVER_URL, "voice", voice_command, "voice"),
                daemon=True
            ).start()
            last_sent_voice = voice_command
            last_sent_voice_time = now
            print(f"[ìŒì„± ì „ì†¡] sent: {voice_command}")

    elif now - last_sent_voice_time >= voice_resend:
        threading.Thread(
            target=send_to_server,
            args=(VOICE_SERVER_URL, "voice", voice_command, "voice"),
            daemon=True
        ).start()
        last_sent_voice_time = now
        print(f"[ìŒì„± ì „ì†¡] re-sent: {voice_command}")


# ê¸°ì¡´ í•¨ìˆ˜ë“¤ (í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
def send_voice_to_server(voice_command):
    """ì„œë²„ì— ìŒì„± ëª…ë ¹ì–´ ì „ì†¡ (UID í¬í•¨, ì‘ë‹µ ìƒì„¸ í™•ì¸)"""
    try_send_voice(voice_command)

def send_gesture_to_server(gesture_label):
    """ì„œë²„ì— ì œìŠ¤ì²˜ ì‹ í˜¸ ì „ì†¡ (UID í¬í•¨, ìŒì„±ê³¼ ë™ì¼í•œ í˜•ì‹)"""
    try_send_gesture(gesture_label)
        
    print(f"{'='*50}")  # êµ¬ë¶„ì„ 

# ================== ì•¡ì…˜ â†’ ì œìŠ¤ì²˜ ë§¤í•‘ ==================
action_to_gesture_map = {
    # ê¸°ì¡´ ëª…ë ¹ì–´ë“¤
    'light_on': 'light_on',
    'light_off': 'light_off',
    'ac_on': 'ac_on',
    'ac_off': 'ac_off',
    'fan_on': 'fan_on',
    'fan_off': 'fan_off',
    'curtain_open': 'curtain_open',
    'curtain_close': 'curtain_close',
    'tv_on': 'tv_on',
    'tv_off': 'tv_off',
    'temp_up': 'small_heart',
    'temp_down': 'small_heart',
    
    # ì„ í’ê¸° ì œì–´ ëª…ë ¹ì–´ë“¤
    'fan_horizontal': 'horizontal',        # ìˆ˜í‰ë°©í–¥ íšŒì „
    'fan_mode': 'mode',                    # ëª¨ë“œ ì „í™˜
    'fan_stronger': 'stronger',            # ë°”ëŒì„¸ê¸°+
    'fan_timer': 'timer',                  # íƒ€ì´ë¨¸ ì„¤ì •
    'fan_vertical': 'vertical',            # ìˆ˜ì§ë°©í–¥ íšŒì „
    'fan_weaker': 'weaker',                # ë°”ëŒì„¸ê¸°-
    
    # ì¡°ëª… ì œì–´ ëª…ë ¹ì–´ë“¤
    'light_10min': 'light_10min',                # íƒ€ì´ë¨¸ 10ë¶„ ì„¤ì •
    'light_2min': 'light_2min',                  # íƒ€ì´ë¨¸ 2ë¶„ ì„¤ì •
    'light_30min': 'light_30min',                # íƒ€ì´ë¨¸ 30ë¶„ ì„¤ì •
    'light_60min': 'light_60min',                # íƒ€ì´ë¨¸ 1ì‹œê°„ ì„¤ì •
    'light_brighter': 'light_brighter',          # ë°ê¸°+
    'light_color': 'light_color',                # ì „ë“±ìƒ‰ ë³€ê²½
    'light_dimmer': 'light_dimmer',              # ë°ê¸°-
    
    # ìƒˆë¡œìš´ ë§¤í•‘ ì¶”ê°€
    'ac_mode': 'ac_mode',                        # ì—ì–´ì»¨ ëª¨ë“œ
    'ac_power': 'ac_power',                      # ì—ì–´ì»¨ ì „ì›
    'ac_temp_down': 'ac_tempDOWN',               # ì—ì–´ì»¨ ì˜¨ë„ ë‹¤ìš´
    'ac_temp_up': 'ac_tempUP',                   # ì—ì–´ì»¨ ì˜¨ë„ ì—…
    'tv_power': 'tv_power',                      # TV ì „ì›
    'tv_channel_up': 'tv_channelUP',             # TV ì±„ë„ ì—…
    'tv_channel_down': 'tv_channelDOWN',         # TV ì±„ë„ ë‹¤ìš´
    'spider_man': 'spider_man',                  # ìŠ¤íŒŒì´ë”ë§¨
    'small_heart': 'small_heart',                # ì‘ì€ í•˜íŠ¸
    'thumbs_down': 'thumbs_down',                # ì—„ì§€ ë‹¤ìš´
    'thumbs_up': 'thumbs_up',                    # ì—„ì§€ ì—…
    'thumbs_left': 'thumbs_left',                # ì—„ì§€ ì™¼ìª½
    'thumbs_right': 'thumbs_right'               # ì—„ì§€ ì˜¤ë¥¸ìª½
}

# TTS (Text-to-Speech) ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import pyttsx3
    TTS_AVAILABLE = True
    print("  TTS ì‚¬ìš© ê°€ëŠ¥ (ìŒì„± ì‘ë‹µ)")
except ImportError:
    TTS_AVAILABLE = False
    print("  TTS ì—†ìŒ - pip install pyttsx3")

# Windows SAPI ë°±ì—… ì‹œë„
try:
    import win32com.client
    SAPI_AVAILABLE = True
    print("  Windows SAPI ì‚¬ìš© ê°€ëŠ¥ (ë°±ì—… TTS)")
except ImportError:
    SAPI_AVAILABLE = False
    print("  Windows SAPI ì—†ìŒ (ì„ íƒì‚¬í•­)")

# ì‚¬ìš´ë“œ ì¬ìƒìš©
try:
    import winsound
    SOUND_AVAILABLE = True
    print("  ì‹œìŠ¤í…œ ì‚¬ìš´ë“œ ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    SOUND_AVAILABLE = False
    print("  ì‹œìŠ¤í…œ ì‚¬ìš´ë“œ ì—†ìŒ")

# ================== ì‚¬ìš´ë“œ ì¬ìƒ ì‹œìŠ¤í…œ ==================
def play_notification_sound(sound_type="system"):
    """ì›¨ì´í¬ì›Œë“œ ê°ì§€ ì‹œ ì•Œë¦¼ìŒ ì¬ìƒ"""
    if not SOUND_AVAILABLE:
        return
    
    try:
        if sound_type == "system":
            # Windows ì‹œìŠ¤í…œ ì•Œë¦¼ìŒ (ë ë¡± ê°™ì€ ì†Œë¦¬)
            winsound.MessageBeep(winsound.MB_OK)
            print("  ì‹œìŠ¤í…œ ì•Œë¦¼ìŒ ì¬ìƒ")
        elif sound_type == "question":
            # ì§ˆë¬¸ ì†Œë¦¬ (ë‹¤ë¥¸ í†¤)
            winsound.MessageBeep(winsound.MB_ICONQUESTION)
            print("  ì§ˆë¬¸ ì•Œë¦¼ìŒ ì¬ìƒ")
        elif sound_type == "beep":
            # ë‹¨ìˆœ ë¹„í”„ìŒ
            winsound.Beep(800, 200)  # 800Hz, 200ms
            print("  ë¹„í”„ìŒ ì¬ìƒ")
        else:
            # ê¸°ë³¸ ì‹œìŠ¤í…œ ì•Œë¦¼ìŒ
            winsound.MessageBeep(-1)
            print("  ê¸°ë³¸ ì•Œë¦¼ìŒ ì¬ìƒ")
    except Exception as e:
        try:
            # ë°±ì—…: ë‹¨ìˆœ ë¹„í”„ìŒ
            winsound.Beep(800, 200)
            print("  ë°±ì—… ë¹„í”„ìŒ ì¬ìƒ")
        except Exception as e2:
            print(f"  ì‚¬ìš´ë“œ ì¬ìƒ ì‹¤íŒ¨: {e2}")

def play_beep_sequence():
    """ì›¨ì´í¬ì›Œë“œ ê°ì§€ ì‹œ íŠ¹ë³„í•œ ë¹„í”„ ì‹œí€€ìŠ¤"""
    if not SOUND_AVAILABLE:
        return
    
    def beep_sequence():
        try:
            # "ë -ë§-ë¡±" ê°™ì€ 3ìŒ ì‹œí€€ìŠ¤ (ë” ì¹œìˆ™í•œ ì†Œë¦¬)
            winsound.Beep(880, 120)   # ë†’ì€ ìŒ (ë )
            time.sleep(0.03)
            winsound.Beep(1100, 120)  # ë” ë†’ì€ ìŒ (ë§) 
            time.sleep(0.03)
            winsound.Beep(660, 250)   # ë‚®ì€ ìŒ (ë¡±)
        except Exception as e:
            print(f"ğŸ”‡ ë©œë¡œë”” ì¬ìƒ ì‹¤íŒ¨: {e}")
    
    try:
        threading.Thread(target=beep_sequence, daemon=True).start()
        print("  ì›¨ì´í¬ì›Œë“œ ë©œë¡œë”” ì¬ìƒ")
    except Exception as e:
        print(f"  ë©œë¡œë”” ìŠ¤ë ˆë“œ ì‹¤íŒ¨: {e}")
        # ë°±ì—…: ê°„ë‹¨í•œ ì•Œë¦¼ìŒ
        play_notification_sound()

# ================== TTS (ìŒì„± ì‘ë‹µ) ì‹œìŠ¤í…œ ==================
class TTSSystem:
    def __init__(self):
        self.engine = None
        self.sapi_engine = None
        self.is_speaking = False
        self.use_sapi = False
        self._initialize()

    def _initialize(self):
        """TTS ì—”ì§„ ì´ˆê¸°í™” (SAPI ìš°ì„ , pyttsx3 ë°±ì—…)"""
        print(f"  TTS ì´ˆê¸°í™” ì‹œì‘...")

        # 1ì°¨ ì‹œë„: Windows SAPI (ë” ì•ˆì •ì )
        if SAPI_AVAILABLE:
            try:
                print("  [1ì°¨] Windows SAPI ì‹œë„...")
                test_sapi = win32com.client.Dispatch("SAPI.SpVoice")
                print("  Windows SAPI ì´ˆê¸°í™” ì„±ê³µ!")
                self.sapi_engine = test_sapi
                self.use_sapi = True
                return
            except Exception as e:
                print(f"  Windows SAPI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        # 2ì°¨ ì‹œë„: pyttsx3
        if TTS_AVAILABLE:
            try:
                print("  [2ì°¨] pyttsx3 ì‹œë„...")
                test_engine = pyttsx3.init()
                test_engine.setProperty('rate', 200)
                test_engine.setProperty('volume', 0.9)
                print("  pyttsx3 ì´ˆê¸°í™” ì„±ê³µ!")
                self.engine = test_engine
                return
            except Exception as e:
                print(f"  pyttsx3 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        print("  ëª¨ë“  TTS ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨!")

    def speak(self, text, async_mode=True):
        """í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜"""
        if not self.use_sapi and not self.engine:
            return

        try:
            print(f"  TTS: '{text}' (ì—”ì§„: {'SAPI' if self.use_sapi else 'pyttsx3'})")
            if async_mode:
                threading.Thread(target=self._speak_sync, args=(text,), daemon=True).start()
            else:
                self._speak_sync(text)
        except Exception as e:
            print(f"  TTS ì˜¤ë¥˜: {e}")

    def _speak_sync(self, text):
        """ë™ê¸° ìŒì„± ì¶œë ¥"""
        try:
            self.is_speaking = True

            # SAPI ì‚¬ìš©
            if self.use_sapi and self.sapi_engine:
                try:
                    self.sapi_engine.Speak(text)
                    self.is_speaking = False
                    return
                except Exception as e:
                    print(f"  SAPI ì¶œë ¥ ì˜¤ë¥˜: {e}")

            # pyttsx3 ì‚¬ìš©
            if self.engine:
                try:
                    self.engine.say(text)
                    self.engine.runAndWait()
                    self.is_speaking = False
                    return
                except Exception as e:
                    print(f"  pyttsx3 ì¶œë ¥ ì˜¤ë¥˜: {e}")

            self.is_speaking = False

        except Exception as e:
            print(f"  TTS ì¶œë ¥ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            self.is_speaking = False

# ================== ì„¤ì • ==================
WAKE_PATTERNS = ["ë¸Œë¦¿ì§€", "ìŠ¤ë§ˆíŠ¸ë¸Œë¦¿ì§€", "ë¸Œë¦¬ì¹˜", "ë¸Œë¦¬ì°Œ", "ì‘ë‹µ", "ìŠ¤ë§ˆíŠ¸"]
WAKE_KEYWORDS = ["ë¸Œë¦¿ì§€", "ë¸Œë¦¬ì¹˜", "ìŠ¤ë§ˆíŠ¸", "ì‘ë‹µ"]
PHRASE_TIME_LIMIT = 2
AMBIENT_DURATION = 0.3

# ================== ì›¨ì´í¬ì›Œë“œ ê°ì§€ ==================
def normalize(text):
    return text.lower().replace(" ", "").replace("-", "")

def detect_wake_word(text):
    norm = normalize(text)
    for kw in WAKE_KEYWORDS:
        if kw in norm:
            print(f"  ë¹ ë¥¸ ë§¤ì¹­: '{kw}' í¬í•¨ë¨")
            return True
    for pattern in WAKE_PATTERNS:
        if Levenshtein.distance(norm, normalize(pattern)) <= 2:
            print(f"  ìœ ì‚¬ ì›¨ì´í¬ì›Œë“œ ê°ì§€: '{text}' â‰ˆ '{pattern}'")
            return True
    return False

# ================== Colabìœ¼ë¡œ ëª…ë ¹ì–´ ì „ì†¡ ==================
def send_to_colab(audio_path, colab_url):
    try:
        print(f"  Colabì— ì˜¤ë””ì˜¤ ì „ì†¡ ì¤‘... â†’ {colab_url}/infer")
        
        # íŒŒì¼ì„ ì˜¬ë°”ë¥´ê²Œ ì—´ê³  ë‹«ê¸°
        with open(audio_path, 'rb') as audio_file:
            files = {'audio': audio_file}
            response = requests.post(f"{colab_url}/infer", files=files)

        print("  ì‘ë‹µ ìƒíƒœ ì½”ë“œ:", response.status_code)
        print("  ì‘ë‹µ ë³¸ë¬¸:", response.text)

        # ë‹¨ìˆœ í…ìŠ¤íŠ¸ íŒŒì‹±: "í…ìŠ¤íŠ¸|ì•¡ì…˜" ë˜ëŠ” "ERROR|ë©”ì‹œì§€"
        response_text = response.text.strip()

        if "|" in response_text:
            parts = response_text.split("|", 1)  # ìµœëŒ€ 1ë²ˆë§Œ ë¶„í• 
            if len(parts) == 2:
                left_part = parts[0].strip()
                right_part = parts[1].strip()

                # ì—ëŸ¬ ì‘ë‹µì¸ì§€ í™•ì¸
                if left_part == "ERROR":
                    return {'error': right_part}
                else:
                    # ì •ìƒ ì‘ë‹µ: "ì¸ì‹í…ìŠ¤íŠ¸|ì•¡ì…˜"
                    return {
                        "text": left_part,
                        "action": right_part
                    }

        # "|"ê°€ ì—†ëŠ” ê²½ìš° (ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ)
        return {
            "text": response_text,
            "action": "none"
        }

    except Exception as e:
        return {'error': str(e)}

# ================== ì˜¤ë””ì˜¤ ë…¹ìŒ ==================
def record_audio(filename="command.wav", duration=3):
    recognizer = sr.Recognizer()
    mic = sr.Microphone()
    with mic as source:
        print("  ëª…ë ¹ì–´ë¥¼ ë§í•˜ì„¸ìš”...")
        recognizer.adjust_for_ambient_noise(source, duration=AMBIENT_DURATION)
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=duration)
        except sr.WaitTimeoutError:
            print("  íƒ€ì„ì•„ì›ƒ: ì‚¬ìš©ìê°€ ë§ì„ ì‹œì‘í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None

        with open(filename, "wb") as f:
            f.write(audio.get_wav_data())
    return filename

# ================== ì›¨ì´í¬ì›Œë“œ ë£¨í”„ ==================
def wait_for_wake_word(recognizer, mic):
    with mic as source:
        print("  ì›¨ì´í¬ì›Œë“œ ëŒ€ê¸° ì¤‘ ('ë¸Œë¦¿ì§€')")
        recognizer.adjust_for_ambient_noise(source, duration=AMBIENT_DURATION)
        audio = recognizer.listen(source, timeout=5, phrase_time_limit=PHRASE_TIME_LIMIT)
        return audio


# í†µí•© ì¸ì‹ ì„¤ì •
INTEGRATED_CONFIG = {
    # ëª¨ë¸ íŒŒì¼
    'mlp_model_pattern': 'MLP_model.pth',
    'tcn_model_pattern': 'TCN_model.pth',
    'mlp_scaler_file': 'MLP_scaler.pkl',
    'tcn_scaler_file': 'TCN_scaler.pkl',
    
    # ì¸ì‹ ì„¤ì • (ë” ìœ ì—°í•˜ê²Œ ì¡°ì •)
    'static_confidence_threshold': 0.7,        # ì •ì  ì œìŠ¤ì²˜ ì‹ ë¢°ë„ ì„ê³„ê°’
    'dynamic_confidence_threshold': 0.6,       # ë™ì  ì œìŠ¤ì²˜ ì‹ ë¢°ë„ ì„ê³„ê°’
    'movement_threshold': 0.02,                # ì›€ì§ì„ ê°ì§€ ì„ê³„ê°’ (ë” ë¯¼ê°í•˜ê²Œ)
    'movement_duration_threshold': 0.3,        # ì›€ì§ì„ ì§€ì† ì‹œê°„ ì„ê³„ê°’ (ë” ì§§ê²Œ)
    'static_hold_time': 1.0,                   # ì •ì  ì œìŠ¤ì²˜ ìœ ì§€ ì‹œê°„ (ì´ˆ)
    'dynamic_sequence_time': 1.0,              # ë™ì  ì‹œí€€ìŠ¤ ìˆ˜ì§‘ ì‹œê°„ (2.0â†’1.0ì´ˆë¡œ ë‹¨ì¶•)
    'dynamic_completion_wait': 1.5,            # ë™ì  ì œìŠ¤ì²˜ ì™„ë£Œ ëŒ€ê¸° ì‹œê°„ (ì‹ ê·œ)
    'prediction_cooldown': 1.0,                # ì˜ˆì¸¡ í›„ ëŒ€ê¸° ì‹œê°„ (ë” ì§§ê²Œ)
    'static_stability_time': 0.3,              # ì •ì  ì•ˆì •í™” ì‹œê°„ (ë” ì§§ê²Œ)
    
    # MediaPipe ì„¤ì •
    'min_detection_confidence': 0.7,
    'min_tracking_confidence': 0.5,
    
    # UI ì„¤ì •
    'fps_display': True,
    'trail_display': True,
    'debug_mode': False,
}

# ìƒ‰ìƒ ì„¤ì • (BGR)
COLORS = {
    'static_mode': (0, 255, 0),       # ì´ˆë¡ìƒ‰ (ì •ì  ëª¨ë“œ)
    'dynamic_mode': (255, 0, 255),    # ë§ˆì  íƒ€ (ë™ì  ëª¨ë“œ)
    'movement': (0, 255, 255),        # ë…¸ë€ìƒ‰ (ì›€ì§ì„ ê°ì§€)
    'processing': (255, 255, 0),      # ì‹œì•ˆ (ì²˜ë¦¬ ì¤‘)
    'predicted': (255, 0, 255),       # ë§ˆì  íƒ€ (ì˜ˆì¸¡ ì™„ë£Œ)
    'waiting': (255, 255, 255),       # í°ìƒ‰ (ëŒ€ê¸°)
    'no_hand': (0, 0, 255),          # ë¹¨ê°„ìƒ‰ (ì† ì—†ìŒ)
    'text': (255, 255, 255),         # í°ìƒ‰ (í…ìŠ¤íŠ¸)
    'bg': (50, 50, 50),              # íšŒìƒ‰ (ë°°ê²½)
    'trail': (0, 165, 255),          # ì£¼í™©ìƒ‰ (ê¶¤ì )
    'good': (0, 255, 0),             # ì´ˆë¡ìƒ‰ (ë†’ì€ ì‹ ë¢°ë„)
    'medium': (0, 255, 255),         # ë…¸ë€ìƒ‰ (ì¤‘ê°„ ì‹ ë¢°ë„)
    'low': (0, 165, 255),            # ì£¼í™©ìƒ‰ (ë‚®ì€ ì‹ ë¢°ë„)
}


class ExistingDataMLP(nn.Module):
    """ê¸°ì¡´ ë°ì´í„°ìš© MLP ëª¨ë¸"""
    
    def __init__(self, input_dim=99, num_classes=15, hidden_sizes=[512, 256, 128, 64], 
                 dropout_rate=0.4, use_batch_norm=True):
        super(ExistingDataMLP, self).__init__()
        
        self.input_dim = input_dim
        self.num_classes = num_classes
        self.use_batch_norm = use_batch_norm
        
        # ì…ë ¥ì¸µ ì •ê·œí™”
        if use_batch_norm:
            self.input_norm = nn.BatchNorm1d(input_dim)
        
        # ë„¤íŠ¸ì›Œí¬ êµ¬ì„±
        layers = []
        prev_size = input_dim
        
        for i, hidden_size in enumerate(hidden_sizes):
            layers.append(nn.Linear(prev_size, hidden_size))
            if use_batch_norm:
                layers.append(nn.BatchNorm1d(hidden_size))
            layers.append(nn.ReLU(inplace=True))
            layers.append(nn.Dropout(dropout_rate))
            prev_size = hidden_size
        
        layers.append(nn.Linear(prev_size, num_classes))
        self.network = nn.Sequential(*layers)
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # ë°°ì¹˜ í¬ê¸°ê°€ 1ì¸ ê²½ìš° BatchNorm ìš°íšŒ
        if x.size(0) == 1 and hasattr(self, 'input_norm'):
            if self.input_norm.running_mean is not None:
                epsilon = self.input_norm.eps
                mean = self.input_norm.running_mean
                var = self.input_norm.running_var
                weight = self.input_norm.weight
                bias = self.input_norm.bias
                
                x_norm = (x - mean) / torch.sqrt(var + epsilon)
                if weight is not None:
                    x_norm = x_norm * weight
                if bias is not None:
                    x_norm = x_norm + bias
                x = x_norm
        else:
            if hasattr(self, 'input_norm'):
                x = self.input_norm(x)
        
        return self.network(x)


class Chomp1d(nn.Module):
    def __init__(self, chomp_size):
        super(Chomp1d, self).__init__()
        self.chomp_size = chomp_size

    def forward(self, x):
        return x[:, :, :-self.chomp_size].contiguous()

class TemporalBlock(nn.Module):
    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2, use_batch_norm=True):
        super(TemporalBlock, self).__init__()
        
        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,
                               stride=stride, padding=padding, dilation=dilation)
        self.chomp1 = Chomp1d(padding)
        self.bn1 = nn.BatchNorm1d(n_outputs) if use_batch_norm else nn.Identity()
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)

        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,
                               stride=stride, padding=padding, dilation=dilation)
        self.chomp2 = Chomp1d(padding)
        self.bn2 = nn.BatchNorm1d(n_outputs) if use_batch_norm else nn.Identity()
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)

        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        self.relu = nn.ReLU()

    def forward(self, x):
        out = self.conv1(x)
        out = self.chomp1(out)
        out = self.bn1(out)
        out = self.relu1(out)
        out = self.dropout1(out)

        out = self.conv2(out)
        out = self.chomp2(out)
        out = self.bn2(out)
        out = self.relu2(out)
        out = self.dropout2(out)

        res = x if self.downsample is None else self.downsample(x)
        return self.relu(out + res)

class TemporalConvNet(nn.Module):
    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2, use_batch_norm=True):
        super(TemporalConvNet, self).__init__()
        layers = []
        num_levels = len(num_channels)
        
        for i in range(num_levels):
            dilation_size = 2 ** i
            in_channels = num_inputs if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            padding = (kernel_size - 1) * dilation_size
            
            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,
                                   padding=padding, dropout=dropout, use_batch_norm=use_batch_norm)]

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

class SequenceTCN(nn.Module):
    def __init__(self, input_features, num_classes, tcn_channels, kernel_size=3, 
                 dropout_rate=0.3, use_skip_connections=True, use_batch_norm=True):
        super(SequenceTCN, self).__init__()
        
        self.input_features = input_features
        self.num_classes = num_classes
        
        if use_batch_norm:
            self.input_norm = nn.BatchNorm1d(input_features)
        
        self.tcn = TemporalConvNet(input_features, tcn_channels, kernel_size, dropout_rate, use_batch_norm)
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        self.classifier = nn.Sequential(
            nn.Linear(tcn_channels[-1], tcn_channels[-1] // 2),
            nn.BatchNorm1d(tcn_channels[-1] // 2) if use_batch_norm else nn.Identity(),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(tcn_channels[-1] // 2, num_classes)
        )
    
    def forward(self, x):
        x = x.transpose(1, 2)
        
        if hasattr(self, 'input_norm'):
            if x.size(0) == 1:
                if self.input_norm.running_mean is not None:
                    epsilon = self.input_norm.eps
                    mean = self.input_norm.running_mean.unsqueeze(0).unsqueeze(-1)
                    var = self.input_norm.running_var.unsqueeze(0).unsqueeze(-1)
                    weight = self.input_norm.weight.unsqueeze(0).unsqueeze(-1) if self.input_norm.weight is not None else None
                    bias = self.input_norm.bias.unsqueeze(0).unsqueeze(-1) if self.input_norm.bias is not None else None
                    
                    x_norm = (x - mean) / torch.sqrt(var + epsilon)
                    if weight is not None:
                        x_norm = x_norm * weight
                    if bias is not None:
                        x_norm = x_norm + bias
                    x = x_norm
            else:
                x = self.input_norm(x)
        
        tcn_out = self.tcn(x)
        pooled = self.global_pool(tcn_out)
        pooled = pooled.squeeze(-1)
        output = self.classifier(pooled)
        
        return output


class IntegratedGestureRecognizer:
    """í†µí•© ì œìŠ¤ì²˜ ì¸ì‹ê¸° (MLP + TCN)"""
    
    def __init__(self, config):
        self.config = config
        self.reset_state()
        
        # ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”©
        self.mlp_model, self.mlp_scaler, self.mlp_labels = self.load_mlp_model()
        self.tcn_model, self.tcn_scaler, self.tcn_labels = self.load_tcn_model()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        if self.mlp_model:
            self.mlp_model = self.mlp_model.to(self.device)
        if self.tcn_model:
            self.tcn_model = self.tcn_model.to(self.device)
        
        # ìƒˆë¡œìš´ ì œìŠ¤ì²˜ ë ˆì´ë¸” ë§¤í•‘ ì¶”ê°€
        self.add_new_gesture_labels()
        
        print(f"  ë””ë°”ì´ìŠ¤: {self.device}")
        
    def reset_state(self):
        """ìƒíƒœ ì´ˆê¸°í™”"""
        self.mode = "static"  # staticì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ë³€ê²½
        self.sequence_buffer = deque(maxlen=60)  # TCNìš© ì‹œí€€ìŠ¤ ë²„í¼
        self.trail_points = deque(maxlen=50)     # ê¶¤ì  í‘œì‹œìš©
        self.static_buffer = deque(maxlen=10)    # ì •ì  ì œìŠ¤ì²˜ ì•ˆì •í™”ìš©
        
        # ì›€ì§ì„ ê°ì§€ (ë” ë¯¼ê°í•œ ê¸°ì¤€)
        self.last_finger_pos = None
        self.movement_history = deque(maxlen=15)  # ë” ì§§ì€ íˆìŠ¤í† ë¦¬ë¡œ ë¹ ë¥¸ ë°˜ì‘
        self.is_moving = False
        self.movement_start_time = 0
        self.static_start_time = time.time()  # ì²˜ìŒì— ì •ì  ì‹œê°„ ì‹œì‘
        self.last_stable_time = time.time()   # ë§ˆì§€ë§‰ìœ¼ë¡œ ì•ˆì •ëœ ì‹œê°„
        self.continuous_movement_time = 0     # ì—°ì† ì›€ì§ì„ ì‹œê°„
        
        # ì˜ˆì¸¡ ê´€ë ¨
        self.last_prediction = None
        self.last_prediction_time = 0
        self.prediction_confidence = 0.0
        self.prediction_source = ""  # "static" or "dynamic"
        
        # í™”ë©´ í‘œì‹œìš© (ì„œë²„ ì „ì†¡ í›„ì—ë„ ìœ ì§€)
        self.display_prediction = None
        self.display_confidence = 0.0
        self.display_source = ""
        self.display_gesture_name = ""
        self.display_time = 0
        
        print("  ìƒíƒœ ë° í™”ë©´ í‘œì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def add_new_gesture_labels(self):
        """ìƒˆë¡œìš´ ì œìŠ¤ì²˜ ë ˆì´ë¸” ë§¤í•‘ ì¶”ê°€"""
        print("  ìƒˆë¡œìš´ ì œìŠ¤ì²˜ ë ˆì´ë¸” ë§¤í•‘ ì¶”ê°€ ì¤‘...")
        
        # MLP ë ˆì´ë¸”ì— ìƒˆë¡œìš´ ì œìŠ¤ì²˜ ì¶”ê°€
        if self.mlp_labels is not None:
            new_mlp_labels = {
                'ac_mode': 'ac_mode',
                'ac_power': 'ac_power', 
                'ac_tempDOWN': 'ac_tempDOWN',
                'ac_tempUP': 'ac_tempUP',
                'tv_power': 'tv_power',
                'tv_channelUP': 'tv_channelUP',
                'tv_channelDOWN': 'tv_channelDOWN',
                'spider_man': 'spider_man',
                'small_heart': 'small_heart',
                'thumbs_down': 'thumbs_down',
                'thumbs_up': 'thumbs_up',
                'thumbs_left': 'thumbs_left',
                'thumbs_right': 'thumbs_right'
            }
            
            # ê¸°ì¡´ ë ˆì´ë¸”ê³¼ ë³‘í•©
            self.mlp_labels.update(new_mlp_labels)
            print(f"     MLP ë ˆì´ë¸”ì— {len(new_mlp_labels)}ê°œ ì œìŠ¤ì²˜ ì¶”ê°€")
        
        # TCN ë ˆì´ë¸”ì— ìƒˆë¡œìš´ ì œìŠ¤ì²˜ ì¶”ê°€
        if self.tcn_labels is not None:
            new_tcn_labels = {
                'ac_mode': 'ac_mode',
                'ac_power': 'ac_power',
                'ac_tempDOWN': 'ac_tempDOWN', 
                'ac_tempUP': 'ac_tempUP',
                'tv_power': 'tv_power',
                'tv_channelUP': 'tv_channelUP',
                'tv_channelDOWN': 'tv_channelDOWN',
                'spider_man': 'spider_man',
                'small_heart': 'small_heart',
                'thumbs_down': 'thumbs_down',
                'thumbs_up': 'thumbs_up',
                'thumbs_left': 'thumbs_left',
                'thumbs_right': 'thumbs_right'
            }
            
            # ê¸°ì¡´ ë ˆì´ë¸”ê³¼ ë³‘í•©
            self.tcn_labels.update(new_tcn_labels)
            print(f"     TCN ë ˆì´ë¸”ì— {len(new_tcn_labels)}ê°œ ì œìŠ¤ì²˜ ì¶”ê°€")
        
        print("     ìƒˆë¡œìš´ ì œìŠ¤ì²˜ ë ˆì´ë¸” ë§¤í•‘ ì™„ë£Œ")
    
    def load_mlp_model(self):
        """MLP ëª¨ë¸ ë¡œë”©"""
        print("  MLP ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        model_files = glob.glob(self.config['mlp_model_pattern'])
        if not model_files:
            print(f"  MLP ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.config['mlp_model_pattern']}")
            return None, None, None
        
        try:
            latest_model = max(model_files, key=os.path.getctime)
            checkpoint = torch.load(latest_model, map_location='cpu')
            config = checkpoint['config']
            
            # ë¼ë²¨ ì •ë³´ ì¶”ì¶œ
            if 'gesture_labels' in checkpoint:
                labels = checkpoint['gesture_labels']
                label_to_name = checkpoint['label_to_name']
            else:
                labels = {}
                label_to_name = {}
            
            # ëª¨ë¸ ìƒì„±
            model = ExistingDataMLP(
                input_dim=config['input_dim'],
                num_classes=config['num_classes'],
                hidden_sizes=config['hidden_sizes'],
                dropout_rate=config['dropout_rate'],
                use_batch_norm=config['use_batch_norm']
            )
            
            model.load_state_dict(checkpoint['model_state_dict'])
            model.eval()
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”©
            with open(self.config['mlp_scaler_file'], 'rb') as f:
                scaler = pickle.load(f)
            
            print(f"     MLP ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            print(f"      - ì •ì  ì œìŠ¤ì²˜: {list(labels.keys())}")
            
            return model, scaler, label_to_name
            
        except Exception as e:
            print(f"     MLP ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None, None, None
    
    def load_tcn_model(self):
        """TCN ëª¨ë¸ ë¡œë”©"""
        print("  TCN ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        model_files = glob.glob(self.config['tcn_model_pattern'])
        if not model_files:
            print(f"  TCN ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.config['tcn_model_pattern']}")
            return None, None, None
        
        try:
            latest_model = max(model_files, key=os.path.getctime)
            checkpoint = torch.load(latest_model, map_location='cpu')
            config = checkpoint['config']
            
            # ë¼ë²¨ ì •ë³´ ì¶”ì¶œ
            if 'gesture_labels' in checkpoint:
                labels = checkpoint['gesture_labels']
                label_to_name = checkpoint['label_to_name']
            elif 'unique_gestures' in checkpoint and 'gesture_to_label' in checkpoint:
                labels = checkpoint['gesture_to_label']
                label_to_name = {v: k for k, v in checkpoint['gesture_to_label'].items()}
            else:
                labels = {}
                label_to_name = {}
            
            # ëª¨ë¸ ìƒì„±
            model = SequenceTCN(
                input_features=config['input_features'],
                num_classes=config['num_classes'],
                tcn_channels=config['tcn_channels'],
                kernel_size=config['kernel_size'],
                dropout_rate=config['dropout_rate'],
                use_skip_connections=config['use_skip_connections'],
                use_batch_norm=config['use_batch_norm']
            )
            
            model.load_state_dict(checkpoint['model_state_dict'])
            model.eval()
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”©
            with open(self.config['tcn_scaler_file'], 'rb') as f:
                scaler = pickle.load(f)
            
            print(f"     TCN ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            print(f"      - ë™ì  ì œìŠ¤ì²˜: {list(labels.keys())}")
            
            return model, scaler, label_to_name
            
        except Exception as e:
            print(f"     TCN ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None, None, None
    
    def detect_movement(self, finger_tip):
        """ê°œì„ ëœ ì›€ì§ì„ ê°ì§€ - ë” ë¯¼ê°í•˜ê³  ìœ ì—°í•œ Dynamic ëª¨ë“œ ì „í™˜"""
        current_time = time.time()
        
        if finger_tip is None:
            # ì†ì´ ì—†ìœ¼ë©´ ì •ì  ëª¨ë“œë¡œ ë³µê·€
            self.is_moving = False
            self.mode = "static"
            self.static_start_time = current_time
            self.last_stable_time = current_time
            return False
        
        if self.last_finger_pos is not None:
            # ì´ì „ ìœ„ì¹˜ì™€ì˜ ê±°ë¦¬ ê³„ì‚°
            distance = np.sqrt((finger_tip[0] - self.last_finger_pos[0])**2 + 
                             (finger_tip[1] - self.last_finger_pos[1])**2)
            
            # ì›€ì§ì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.movement_history.append(distance)
            
            # ë” ì§§ì€ ìœˆë„ìš°ë¡œ ë¹ ë¥¸ ë°˜ì‘
            if len(self.movement_history) >= 5:  # 10ì—ì„œ 5ë¡œ ì¤„ì„
                avg_movement = np.mean(list(self.movement_history)[-8:])   # ìµœê·¼ 8í”„ë ˆì„
                max_movement = np.max(list(self.movement_history)[-3:])    # ìµœê·¼ 3í”„ë ˆì„ ìµœëŒ€ê°’
                recent_movement = np.mean(list(self.movement_history)[-3:])  # ìµœê·¼ 3í”„ë ˆì„ í‰ê· 
                
                # ì›€ì§ì„ ê°ì§€ ê¸°ì¤€ ì™„í™” (OR ì¡°ê±´ìœ¼ë¡œ ë” ë¯¼ê°í•˜ê²Œ)
                is_significant_movement = (
                    avg_movement > self.config['movement_threshold'] or 
                    max_movement > self.config['movement_threshold'] * 1.2 or
                    recent_movement > self.config['movement_threshold'] * 0.8  # ì¶”ê°€ ì¡°ê±´
                )
                
                if is_significant_movement:
                    if not self.is_moving:
                        # ì›€ì§ì„ ì‹œì‘
                        self.movement_start_time = current_time
                        self.is_moving = True
                        print(f"  ì›€ì§ì„ ê°ì§€ ì‹œì‘: avg={avg_movement:.3f}, max={max_movement:.3f}")
                    
                    # ì—°ì† ì›€ì§ì„ ì‹œê°„ ê³„ì‚°
                    self.continuous_movement_time = current_time - self.movement_start_time
                    
                    # ë” ì§§ì€ ì‹œê°„ìœ¼ë¡œ Dynamic ëª¨ë“œ ì§„ì… (1ì´ˆ â†’ 0.5ì´ˆ)
                    if self.continuous_movement_time >= self.config['movement_duration_threshold']:
                        if self.mode != "dynamic":
                            self.mode = "dynamic"
                            print(f"  Dynamic ëª¨ë“œ ì§„ì… ({self.continuous_movement_time:.1f}ì´ˆ)")
                        return True
                    
                else:
                    # ì›€ì§ì„ì´ ë©ˆì¶¤
                    if self.is_moving:
                        self.is_moving = False
                        self.last_stable_time = current_time
                        print(f"  ì›€ì§ì„ ì •ì§€ (ì§€ì†ì‹œê°„: {self.continuous_movement_time:.1f}ì´ˆ)")
                        
                        # Dynamic ëª¨ë“œì˜€ë‹¤ë©´ Staticìœ¼ë¡œ ë³µê·€í•˜ê¸° ì „ ì ì‹œ ëŒ€ê¸°
                        if self.mode == "dynamic":
                            # Dynamic ì˜ˆì¸¡ì„ ìœ„í•œ ì‹œê°„ í™•ë³´
                            pass
                        else:
                            # ì¦‰ì‹œ Static ëª¨ë“œë¡œ
                            self.mode = "static"
                            self.static_start_time = current_time
                    
                    # Static ëª¨ë“œ ë³µê·€ ì¡°ê±´ (ë” ë¹ ë¥´ê²Œ)
                    stable_duration = current_time - self.last_stable_time
                    if stable_duration >= self.config['static_stability_time']:
                        if self.mode != "static":
                            self.mode = "static"
                            self.static_start_time = current_time
                            print(f"  Static ëª¨ë“œ ë³µê·€ (ì•ˆì •í™”: {stable_duration:.1f}ì´ˆ)")
                    
                    return False
        
        self.last_finger_pos = finger_tip
        return False
    
    def add_frame(self, mlp_features, tcn_features, finger_tip=None, hand_detected=False):
        """í”„ë ˆì„ ì¶”ê°€ (MLPìš©ê³¼ TCNìš© íŠ¹ì§•ì„ ê°ê° ì²˜ë¦¬)"""
        current_time = time.time()
        
        if not hand_detected or mlp_features is None:
            # ì†ì´ ì—†ì„ ë•Œ ìƒíƒœ ì´ˆê¸°í™”
            self.last_finger_pos = None
            self.movement_history.clear()
            self.is_moving = False
            return
        
        # ì›€ì§ì„ ê°ì§€
        is_moving = self.detect_movement(finger_tip)
        
        # ê¶¤ì  í¬ì¸íŠ¸ ì¶”ê°€
        if finger_tip is not None:
            self.trail_points.append(finger_tip)
        
        # ì‹œí€€ìŠ¤ ë²„í¼ì— TCNìš© íŠ¹ì§• ì¶”ê°€
        if tcn_features is not None:
            self.sequence_buffer.append(tcn_features)
        
        # ì •ì  ì œìŠ¤ì²˜ìš© ë²„í¼ì— MLPìš© íŠ¹ì§• ì¶”ê°€ (ì›€ì§ì„ì´ ì ì„ ë•Œ)
        if not is_moving:
            self.static_buffer.append(mlp_features)
    
    def should_predict_static(self):
        """ì •ì  ì œìŠ¤ì²˜ ì˜ˆì¸¡ ì—¬ë¶€ - Static ëª¨ë“œ ìš°ì„ """
        current_time = time.time()
        
        # ì¡°ê±´: Static ëª¨ë“œ + ì¶©ë¶„í•œ ë°ì´í„° + ì¼ì • ì‹œê°„ ìœ ì§€
        is_static_mode = self.mode == "static"
        has_data = len(self.static_buffer) >= 5
        not_moving = not self.is_moving
        held_long_enough = (current_time - self.static_start_time) >= self.config['static_hold_time']
        cooldown_passed = (current_time - self.last_prediction_time) >= self.config['prediction_cooldown']
        
        return is_static_mode and has_data and not_moving and held_long_enough and cooldown_passed
    
    def should_predict_dynamic(self):
        """ë™ì  ì œìŠ¤ì²˜ ì˜ˆì¸¡ ì—¬ë¶€ - ì¤‘ë³µ ì˜ˆì¸¡ ë°©ì§€ ê°•í™”"""
        current_time = time.time()
        
        # ì¡°ê±´: Dynamic ëª¨ë“œì˜€ë˜ ê²½í—˜ + ì¶©ë¶„í•œ ì‹œí€€ìŠ¤ ë°ì´í„° + ì›€ì§ì„ íŒ¨í„´ ì™„ë£Œ
        was_dynamic_mode = self.mode == "dynamic" or (self.movement_start_time > 0 and self.continuous_movement_time >= self.config['movement_duration_threshold'])
        has_data = len(self.sequence_buffer) >= 60
        
        # ê°œì„ ëœ ì›€ì§ì„ ì™„ë£Œ ì¡°ê±´: ì†ì´ í™”ë©´ì— ìˆì–´ë„ ì¼ì • ì‹œê°„ ì›€ì§ì„ì´ ì—†ìœ¼ë©´ OK
        movement_pattern_complete = (
            # ê¸°ì¡´: ì™„ì „íˆ ë©ˆì¶¤ (ì†ì´ ë‚˜ê°„ ê²½ìš°)
            (self.is_moving == False and self.movement_start_time > 0) or
            # ì‹ ê·œ: ì†ì´ ìˆì§€ë§Œ ì¶©ë¶„íˆ ì˜¤ë˜ ì•ˆì •ëœ ê²½ìš° (1ì´ˆ)
            (not self.is_moving and (current_time - self.last_stable_time) >= 1.0) or
            # ì‹ ê·œ: Dynamic ëª¨ë“œì—ì„œ ì¶©ë¶„í•œ ì‹œê°„ì´ ì§€ë‚¨
            (self.mode == "dynamic" and (current_time - self.movement_start_time) >= self.config['dynamic_completion_wait'])
        )
        
        # ì‹œí€€ìŠ¤ ì‹œê°„ ì¡°ê±´ ì™„í™”
        sequence_time = (current_time - self.last_stable_time) >= self.config['dynamic_sequence_time']
        cooldown_passed = (current_time - self.last_prediction_time) >= self.config['prediction_cooldown']
        significant_movement = self.continuous_movement_time >= self.config['movement_duration_threshold']
        
        # ì¤‘ë³µ ë°©ì§€: ìµœê·¼ì— ê°™ì€ ì†ŒìŠ¤ë¡œ ì˜ˆì¸¡í–ˆë‹¤ë©´ ë” ì˜¤ëœ ì‹œê°„ ëŒ€ê¸°
        if self.prediction_source == "dynamic":
            extended_cooldown = (current_time - self.last_prediction_time) >= (self.config['prediction_cooldown'] * 3)
            return was_dynamic_mode and has_data and movement_pattern_complete and extended_cooldown and significant_movement
        
        return was_dynamic_mode and has_data and movement_pattern_complete and cooldown_passed and significant_movement
    
    def predict_static(self):
        """ì •ì  ì œìŠ¤ì²˜ ì˜ˆì¸¡"""
        if not self.mlp_model or len(self.static_buffer) == 0:
            return None, 0.0
        
        try:
            # ìµœê·¼ í”„ë ˆì„ë“¤ì˜ í‰ê·  ì‚¬ìš©
            features = np.mean(list(self.static_buffer), axis=0)
            features_scaled = self.mlp_scaler.transform(features.reshape(1, -1))
            features_tensor = torch.FloatTensor(features_scaled).to(self.device)
            
            with torch.no_grad():
                outputs = self.mlp_model(features_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                confidence, predicted = torch.max(probabilities, 1)
                
                predicted_class = predicted.item()
                confidence_score = confidence.item()
            
            return predicted_class, confidence_score
            
        except Exception as e:
            if self.config['debug_mode']:
                print(f"ì •ì  ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            return None, 0.0
    
    def predict_dynamic(self):
        """ë™ì  ì œìŠ¤ì²˜ ì˜ˆì¸¡"""
        if not self.tcn_model or len(self.sequence_buffer) < 60:
            return None, 0.0
        
        try:
            # ë§ˆì§€ë§‰ 60í”„ë ˆì„ ì‚¬ìš©
            sequence = np.array(list(self.sequence_buffer)[-60:])
            sequence_scaled = self.tcn_scaler.transform(sequence)
            sequence_tensor = torch.FloatTensor(sequence_scaled).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                outputs = self.tcn_model(sequence_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                confidence, predicted = torch.max(probabilities, 1)
                
                predicted_class = predicted.item()
                confidence_score = confidence.item()
            
            return predicted_class, confidence_score
            
        except Exception as e:
            if self.config['debug_mode']:
                print(f"ë™ì  ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            return None, 0.0
    
    def update_and_predict(self):
        """ì—…ë°ì´íŠ¸ ë° ì˜ˆì¸¡ ì‹¤í–‰"""
        current_time = time.time()
        
        # ì •ì  ì œìŠ¤ì²˜ ì˜ˆì¸¡ ì‹œë„
        if self.should_predict_static():
            prediction, confidence = self.predict_static()
            
            if prediction is not None and confidence >= self.config['static_confidence_threshold']:
                gesture_name = self.mlp_labels.get(prediction, f'static_{prediction}')
                self.set_prediction(prediction, confidence, "static", gesture_name)
                return True
        
        # ë™ì  ì œìŠ¤ì²˜ ì˜ˆì¸¡ ì‹œë„
        if self.should_predict_dynamic():
            prediction, confidence = self.predict_dynamic()
            
            if prediction is not None and confidence >= self.config['dynamic_confidence_threshold']:
                gesture_name = self.tcn_labels.get(prediction, f'dynamic_{prediction}')
                self.set_prediction(prediction, confidence, "dynamic", gesture_name)
                return True
        
        return False
    
    def set_prediction(self, prediction, confidence, source, gesture_name):
        """ì˜ˆì¸¡ ê²°ê³¼ ì„¤ì •"""
        current_time = time.time()
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì„¤ì •
        self.last_prediction = prediction
        self.prediction_confidence = confidence
        self.prediction_source = source
        self.last_prediction_time = current_time
        
        # í™”ë©´ í‘œì‹œìš© (ì§€ì†ì  í‘œì‹œ)
        self.display_prediction = prediction
        self.display_confidence = confidence
        self.display_source = source
        self.display_gesture_name = gesture_name
        self.display_time = current_time
        
        # ìƒíƒœ ì¼ë¶€ ì´ˆê¸°í™”
        if source == "static":
            self.static_buffer.clear()
        elif source == "dynamic":
            # ë™ì  ì œìŠ¤ì²˜ ì¸ì‹ í›„ ìƒíƒœ ì™„ì „ ì´ˆê¸°í™”
            self.sequence_buffer.clear()  # ì‹œí€€ìŠ¤ ë²„í¼ ì´ˆê¸°í™”
            self.movement_start_time = 0
            self.continuous_movement_time = 0
            self.is_moving = False
            self.last_stable_time = current_time
            self.mode = "static"  # Static ëª¨ë“œë¡œ ê°•ì œ ì „í™˜
            self.static_start_time = current_time
            # ê¶¤ì  ì™„ì „ ì´ˆê¸°í™”
            self.trail_points.clear()
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥ (ë” ìƒì„¸í•œ ì •ë³´ í¬í•¨)
        source_emoji = "ğŸ›‘" if source == "static" else "ğŸŒ€"
        mode_info = f"[{self.mode.upper()}]" if source == "dynamic" else ""
        print(f" {source_emoji} {source.upper()} ì¸ì‹: {gesture_name.upper()} ({confidence:.1%}) {mode_info}")
        
        # ë™ì  ì œìŠ¤ì²˜ ì¸ì‹ ì‹œ ì¶”ê°€ ì •ë³´
        if source == "dynamic" and self.config.get('debug_mode', False):
            print(f"    ì‹œí€€ìŠ¤ ê¸¸ì´: {len(self.sequence_buffer)}/60")
            print(f"    ì›€ì§ì„ ì‹œê°„: {self.continuous_movement_time:.1f}ì´ˆ")
            print(f"    ì•ˆì •í™” ì‹œê°„: {current_time - self.last_stable_time:.1f}ì´ˆ")
    
    def get_status(self):
        """í˜„ì¬ ìƒíƒœ ë°˜í™˜"""
        current_time = time.time()
        
        if self.mode == "dynamic":
            if self.is_moving:
                elapsed = current_time - self.movement_start_time
                return "DYNAMIC_MODE", f"Movement: {elapsed:.1f}s"
            else:
                wait_time = current_time - self.last_stable_time
                return "DYNAMIC_MODE", f"Processing: {wait_time:.1f}s"
        elif self.mode == "static":
            if len(self.static_buffer) > 0:
                held_time = current_time - self.static_start_time
                return "STATIC_MODE", f"Held: {held_time:.1f}s"
            else:
                return "STATIC_MODE", "Ready for gesture"
        else:
            return "MONITORING", "Detecting mode..."


def extract_hand_landmarks(image, hands_detector):
    """ì† ëœë“œë§ˆí¬ ì¶”ì¶œ (test_existing_mlp_live.pyì™€ ì™„ì „ ë™ì¼)"""
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = hands_detector.process(image_rgb)
    
    landmarks = []
    handedness = None
    confidence = 0.0
    finger_tip = None
    
    if results.multi_hand_landmarks and results.multi_handedness:
        # ì²« ë²ˆì§¸ ì†ë§Œ ì‚¬ìš©
        hand_landmarks = results.multi_hand_landmarks[0]
        hand_info = results.multi_handedness[0]
        
        # ì† ì •ë³´ ì¶”ì¶œ
        handedness = hand_info.classification[0].label  # 'Left' or 'Right'
        confidence = hand_info.classification[0].score
        
        # 21ê°œ ê´€ì ˆ ì¢Œí‘œ ì¶”ì¶œ (x, y, z, visibility) - test_existing_mlp_live.pyì™€ ë™ì¼
        joint = np.zeros((21, 4))
        for j, lm in enumerate(hand_landmarks.landmark):
            joint[j] = [lm.x, lm.y, lm.z, lm.visibility]
        
        landmarks = joint
        
        # ê²€ì§€ ë ì¢Œí‘œ (ê¶¤ì ìš©) - 8ë²ˆì§¸ ëœë“œë§ˆí¬
        finger_tip = (joint[8, 0], joint[8, 1])  # x, y ì¢Œí‘œ
    
    return landmarks, handedness, confidence, finger_tip

def create_features_from_landmarks(landmarks):
    """ëœë“œë§ˆí¬ì—ì„œ MLPìš© íŠ¹ì§• ë²¡í„° ìƒì„± (test_existing_mlp_live.pyì™€ ì™„ì „ ë™ì¼)"""
    if len(landmarks) == 0:
        return None
    
    try:
        # ê´€ì ˆ ì¢Œí‘œ (21 x 4 = 84ì°¨ì›) - test_existing_mlp_live.pyì™€ ë™ì¼
        joint = np.array(landmarks)
        
        # ë²¡í„° ê³„ì‚° (data_collect_improved.pyì™€ ë™ì¼í•œ ë°©ì‹)
        v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3]
        v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3]
        v = v2 - v1
        
        # ì •ê·œí™”
        norms = np.linalg.norm(v, axis=1)
        norms[norms == 0] = 1e-6  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        v = v / norms[:, np.newaxis]
        
        # ê´€ì ˆ ê°„ ê°ë„ ê³„ì‚°
        angle = np.arccos(np.clip(np.einsum('nt,nt->n',
            v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:],
            v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:]), -1, 1))
        
        angle = np.degrees(angle)
        
        # íŠ¹ì§• ë²¡í„° ìƒì„±: ê´€ì ˆ ìœ„ì¹˜(84) + ê°ë„(15) = 99ì°¨ì›
        features = np.concatenate([joint.flatten(), angle])
        
        # ìœ íš¨ì„± ê²€ì‚¬
        if np.isnan(features).any() or np.isinf(features).any():
            return None
        
        return features
        
    except Exception as e:
        return None

def create_tcn_features_from_landmarks(landmarks_joint):
    """ëœë“œë§ˆí¬ì—ì„œ TCNìš© íŠ¹ì§• ë²¡í„° ìƒì„± (collect_sequence_data.pyì™€ ë™ì¼)"""
    if len(landmarks_joint) == 0:
        return None
    
    try:
        # 21ê°œ ê´€ì ˆ ì¢Œí‘œ (x,y,z)ë§Œ ì¶”ì¶œ
        joint = np.array(landmarks_joint)[:, :3]  # (21, 3)
        
        # ê¸°ë³¸ ì¢Œí‘œì— visibility ì¶”ê°€ (1.0ìœ¼ë¡œ ì„¤ì •)
        joint_with_vis = np.column_stack([joint, np.ones(21)])
        
        # ë²¡í„° ê³„ì‚° (ê¸°ì¡´ ë°©ì‹ê³¼ ë™ì¼)
        v1 = joint_with_vis[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3]
        v2 = joint_with_vis[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3]
        v = v2 - v1
        
        # ì •ê·œí™”
        norms = np.linalg.norm(v, axis=1)
        norms[norms == 0] = 1e-6
        v = v / norms[:, np.newaxis]
        
        # ê´€ì ˆ ê°„ ê°ë„ ê³„ì‚°
        angle = np.arccos(np.clip(np.einsum('nt,nt->n',
            v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:],
            v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:]), -1, 1))
        
        angle = np.degrees(angle)
        
        # íŠ¹ì§• ë²¡í„°: ê´€ì ˆ(84) + ê°ë„(15) = 99ì°¨ì›
        features = np.concatenate([joint_with_vis.flatten(), angle])
        
        # ìœ íš¨ì„± ê²€ì‚¬
        if np.isnan(features).any() or np.isinf(features).any():
            return None
        
        return features
        
    except Exception as e:
        return None


def draw_landmarks_and_trail(image, landmarks, finger_tip, trail_points, handedness, confidence, mode):
    """ì† ëœë“œë§ˆí¬ì™€ ê¶¤ì  ê·¸ë¦¬ê¸°"""
    h, w = image.shape[:2]
    
    if len(landmarks) > 0:
        # ëª¨ë“œì— ë”°ë¥¸ ìƒ‰ìƒ
        if mode == "STATIC_MODE":
            color = COLORS['static_mode']
            hand_text = f"{handedness} Hand (Static)"
        elif mode == "DYNAMIC_MODE":
            color = COLORS['dynamic_mode']  
            hand_text = f"{handedness} Hand (Dynamic)"
        else:
            color = COLORS['waiting']
            hand_text = f"{handedness} Hand"
        
        # ê²€ì§€ ë ê°•ì¡°
        if finger_tip is not None:
            finger_x = int(finger_tip[0] * w)
            finger_y = int(finger_tip[1] * h)
            cv2.circle(image, (finger_x, finger_y), 8, color, -1)
            cv2.circle(image, (finger_x, finger_y), 12, color, 2)
        
        # ì† ì •ë³´ í‘œì‹œ (í¬ê¸° ì¶•ì†Œ)
        cv2.putText(image, f"{hand_text} ({confidence:.2f})", 
                   (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)  # í¬ê¸°ì™€ ë‘ê»˜ ì¶•ì†Œ
    
    # ê¶¤ì  ê·¸ë¦¬ê¸°
    if INTEGRATED_CONFIG['trail_display'] and len(trail_points) > 1:
        points = [(int(x * w), int(y * h)) for x, y in trail_points]
        for i in range(1, len(points)):
            thickness = max(1, int(3 * (i / len(points))))
            cv2.line(image, points[i-1], points[i], COLORS['trail'], thickness)
    
    return image

def draw_integrated_ui(image, recognizer, fps=None):
    h, w = image.shape[:2]
    
    # í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
    status, status_detail = recognizer.get_status()
    
    # ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ
    if status == "STATIC_MODE":
        status_color = COLORS['static_mode']
        status_text = " STATIC MODE"
    elif status == "DYNAMIC_MODE":
        status_color = COLORS['dynamic_mode']
        status_text = " DYNAMIC MODE"
    elif status == "MONITORING":
        status_color = COLORS['waiting']
        status_text = " MONITORING"
    else:
        status_color = COLORS['waiting']
        status_text = "âš¡ AUTO MODE"
    
    # ì¸ì‹ëœ ì œìŠ¤ì²˜ í° í™”ë©´ ì¤‘ì•™ í‘œì‹œ (í™”ë©´ í‘œì‹œìš© ë³€ìˆ˜ ì‚¬ìš©)
    current_time = time.time()
    show_prediction = False
    prediction_text = ""
    pred_color = COLORS['text']
    
    if (recognizer.display_prediction is not None and 
        (current_time - recognizer.display_time) < 3.0 and  # 3ì´ˆê°„ í‘œì‹œ
        recognizer.display_gesture_name.lower() != 'nothing'):  # nothingì€ í™”ë©´ì— í‘œì‹œí•˜ì§€ ì•ŠìŒ
        
        show_prediction = True
        if recognizer.display_source == "static":
            prediction_text = f"ğŸ›‘ {recognizer.display_gesture_name.upper()}"
        else:
            prediction_text = f"ğŸŒ€ {recognizer.display_gesture_name.upper()}"
        
        confidence = recognizer.display_confidence
        if confidence >= 0.8:
            pred_color = COLORS['good']
        elif confidence >= 0.6:
            pred_color = COLORS['medium']
        else:
            pred_color = COLORS['low']
    
    # í° ì œìŠ¤ì²˜ í‘œì‹œ (í™”ë©´ ì¤‘ì•™ ìƒë‹¨) - ê¸€ì”¨ í¬ê¸° ì¶•ì†Œ
    if show_prediction:
        # ë°°ê²½ ë°•ìŠ¤
        overlay = image.copy()
        text_size = cv2.getTextSize(prediction_text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 2)[0]  # í¬ê¸° ì¶•ì†Œ
        box_w = text_size[0] + 30
        box_h = 60  # ë†’ì´ ì¶•ì†Œ
        box_x = (w - box_w) // 2
        box_y = 40  # ìœ„ì¹˜ ì¡°ì •
        
        cv2.rectangle(overlay, (box_x, box_y), (box_x + box_w, box_y + box_h), COLORS['bg'], -1)
        cv2.addWeighted(overlay, 0.8, image, 0.2, 0, image)
        
        # ì œìŠ¤ì²˜ í…ìŠ¤íŠ¸ 
        text_x = box_x + 15
        text_y = box_y + 30
        cv2.putText(image, prediction_text, (text_x, text_y), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, pred_color, 2)  # í¬ê¸°ì™€ ë‘ê»˜ ì¶•ì†Œ
        
        # ì‹ ë¢°ë„ í‘œì‹œ 
        conf_text = f"{recognizer.display_confidence:.1%}"
        cv2.putText(image, conf_text, (text_x, text_y + 20), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, pred_color, 1)  # í¬ê¸°ì™€ ë‘ê»˜ ì¶•ì†Œ
    
    # í•˜ë‹¨ ìƒíƒœ ì •ë³´ ë°•ìŠ¤ 
    overlay = image.copy()
    cv2.rectangle(overlay, (10, h-120), (w-10, h-10), COLORS['bg'], -1)  # ë†’ì´ ì¶•ì†Œ
    cv2.addWeighted(overlay, 0.7, image, 0.3, 0, image)
    
    # ìƒíƒœ í…ìŠ¤íŠ¸ 
    cv2.putText(image, status_text, 
               (20, h-90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 1)  # í¬ê¸°ì™€ ë‘ê»˜ ì¶•ì†Œ
    
    # ìƒíƒœ ì„¸ë¶€ ì •ë³´ 
    cv2.putText(image, status_detail, 
               (20, h-70), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS['text'], 1)  # í¬ê¸° ì¶•ì†Œ
    
    # ìµœê·¼ ì¸ì‹ ê²°ê³¼ 
    if show_prediction:
        recent_text = f"Recent: {prediction_text} ({recognizer.display_confidence:.1%})"
        cv2.putText(image, recent_text, 
                   (20, h-50), cv2.FONT_HERSHEY_SIMPLEX, 0.4, pred_color, 1)  # í¬ê¸° ì¶•ì†Œ
    
    # ë²„í¼ ì •ë³´ 
    static_count = len(recognizer.static_buffer)
    dynamic_count = len(recognizer.sequence_buffer)
    buffer_text = f"Buffer: Static({static_count}/10) Dynamic({dynamic_count}/60)"
    cv2.putText(image, buffer_text, 
               (20, h-35), cv2.FONT_HERSHEY_SIMPLEX, 0.35, COLORS['text'], 1)  # í¬ê¸° ì¶•ì†Œ
    
    # FPS í‘œì‹œ 
    if fps is not None and INTEGRATED_CONFIG['fps_display']:
        cv2.putText(image, f"FPS: {fps:.1f}", 
                   (w-80, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS['text'], 1)  # í¬ê¸°ì™€ ìœ„ì¹˜ ì¡°ì •
    
    # ì œì–´ ê°€ì´ë“œ 
    cv2.putText(image, "Controls: R-Reset, D-Debug, Q-Quit | Voice: Wake+Command", 
               (20, h-15), cv2.FONT_HERSHEY_SIMPLEX, 0.35, COLORS['text'], 1)  # í¬ê¸° ì¶•ì†Œ
    
    return image


class VoiceRecognitionThread(threading.Thread):
    """ìŒì„± ì¸ì‹ ì „ìš© ìŠ¤ë ˆë“œ """
    
    def __init__(self, colab_url=""):
        super().__init__()
        self.colab_url = colab_url
        self.running = True
        self.daemon = True
        
        # TTS ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.tts_system = TTSSystem()
        
        # ìŒì„± ì¸ì‹ ì´ˆê¸°í™”
        self.recognizer = sr.Recognizer()
        self.mic = sr.Microphone()
        
        print(" ìŒì„± ì¸ì‹ ìŠ¤ë ˆë“œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def run(self):
        print(" ìŒì„± ì¸ì‹ ìŠ¤ë ˆë“œ ì‹œì‘")
        
        # ì´ˆê¸° ì§€ì—°ìœ¼ë¡œ ë©”ì¸ ìŠ¤ë ˆë“œì™€ ì¶©ëŒ ë°©ì§€
        time.sleep(2)
        
        if self.tts_system.engine or self.tts_system.use_sapi:
            self.tts_system.speak("ìŒì„± ì œì–´ ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤", async_mode=False)
        
        while self.running:
            try:
                # íƒ€ì„ì•„ì›ƒ ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€
                try:
                    audio = wait_for_wake_word(self.recognizer, self.mic)
                except sr.WaitTimeoutError:
                    # íƒ€ì„ì•„ì›ƒì€ ì •ìƒì ì¸ ìƒí™© - ê³„ì† ëŒ€ê¸°
                    continue
                except Exception as e:
                    print(f" ì›¨ì´í¬ì›Œë“œ ëŒ€ê¸° ì˜¤ë¥˜: {e}")
                    time.sleep(1)
                    continue
                
                wav = audio.get_wav_data(convert_rate=16000, convert_width=2)
                with io.BytesIO(wav) as wav_io:
                    with wave.open(wav_io, 'rb') as wav_file:
                        frames = wav_file.readframes(wav_file.getnframes())
                        audio_np = np.frombuffer(frames, dtype=np.int16).astype(np.float32) / 32768.0

                # Whisper ì—†ì´ Google STTë¡œ ê°„ë‹¨ ì¸ì‹ (ì›¨ì´í¬ì›Œë“œ ê°ì§€ìš©)
                try:
                    text = self.recognizer.recognize_google(audio, language='ko-KR')
                    print(f" ì¸ì‹ë¨: {text}")
                except sr.UnknownValueError:
                    print(" ì¸ì‹ ì‹¤íŒ¨ (ë¬´ìŒ ë˜ëŠ” ì¡ìŒ)")
                    continue
                except sr.RequestError as e:
                    print(f" Google STT ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
                    continue

                if detect_wake_word(text):
                    print(" ì›¨ì´í¬ì›Œë“œ ê°ì§€ë¨ â†’ ëª…ë ¹ì–´ ë…¹ìŒìœ¼ë¡œ ì „í™˜")

                    # ì›¨ì´í¬ì›Œë“œ ê°ì§€ ì•Œë¦¼ìŒ ì¬ìƒ (TTSë³´ë‹¤ ë¨¼ì €)
                    play_beep_sequence()
                    
                    # ì›¨ì´í¬ì›Œë“œ ì¸ì‹ TTS ì‘ë‹µ
                    if self.tts_system and (self.tts_system.engine or self.tts_system.use_sapi):
                        self.tts_system.speak("ë„¤, ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?", async_mode=False)

                    cmd_audio = record_audio()
                    if cmd_audio is None:
                        continue
                    
                    if self.colab_url:
                        result = send_to_colab(cmd_audio, self.colab_url)

                        if "error" in result:
                            print(" ì˜¤ë¥˜:", result["error"])
                            if self.tts_system and (self.tts_system.engine or self.tts_system.use_sapi):
                                self.tts_system.speak("ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", async_mode=False)
                        else:
                            print("âœ… ì¸ì‹ í…ìŠ¤íŠ¸:", result["text"])
                            print("ğŸ® ë§¤ì¹­ëœ ëª…ë ¹ì–´:", result["action"])
                            
                            action = result.get("action", "")
                            
                            # ì„œë²„ì— ìŒì„± ëª…ë ¹ ì „ì†¡ (ìƒˆë¡œìš´ ì¿¨ë‹¤ìš´ ì‹œìŠ¤í…œ ì ìš©)
                            if action and action in action_to_gesture_map:
                                gesture_command = action_to_gesture_map[action]
                                print(f"\n [ìŒì„±] ëª…ë ¹ì–´ ì¸ì‹ë¨: '{result['text']}'")
                                print(f" ì œìŠ¤ì²˜ ë§¤í•‘: {action} â†’ {gesture_command}")
                                print(f" ì„œë²„ ì „ì†¡ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
                                
                                # ìƒˆë¡œìš´ ì¿¨ë‹¤ìš´ ì‹œìŠ¤í…œìœ¼ë¡œ ì „ì†¡
                                try_send_voice(gesture_command)
                                
                            # ìƒˆë¡œìš´ ëª…ë ¹ì–´ ì§ì ‘ ë§¤í•‘
                            elif action in ['ac_mode', 'ac_power', 'ac_tempDOWN', 'ac_tempUP', 'tv_power', 'tv_channelUP', 'tv_channelDOWN', 'spider_man', 'small_heart', 'thumbs_down', 'thumbs_up', 'thumbs_left', 'thumbs_right']:
                                print(f"\n [ìŒì„±] ìƒˆë¡œìš´ ëª…ë ¹ì–´ ì¸ì‹ë¨: '{result['text']}'")
                                print(f" ì§ì ‘ ë§¤í•‘: {action}")
                                print(f" ì„œë²„ ì „ì†¡ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
                                
                                # ìƒˆë¡œìš´ ì¿¨ë‹¤ìš´ ì‹œìŠ¤í…œìœ¼ë¡œ ì „ì†¡
                                try_send_voice(action)
                                
                            elif action:
                                # ë§¤í•‘ë˜ì§€ ì•Šì€ ì•¡ì…˜ë„ ì§ì ‘ ì „ì†¡
                                print(f"\n [ìŒì„±] ëª…ë ¹ì–´ ì¸ì‹ë¨: '{result['text']}'")
                                print(f" ì§ì ‘ ì „ì†¡: {action}")
                                print(f" ì£¼ì˜: '{action}'ì€ ì œìŠ¤ì²˜ ë§¤í•‘ì— ì—†ìŠµë‹ˆë‹¤.")
                                
                                # ìƒˆë¡œìš´ ì¿¨ë‹¤ìš´ ì‹œìŠ¤í…œìœ¼ë¡œ ì „ì†¡
                                try_send_voice(action)
                                
                            else:
                                print(f" ì•¡ì…˜ì´ ë¹„ì–´ìˆê±°ë‚˜ ì¸ì‹ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: '{action}'")
                            
                            # TTSë¡œ ëª…ë ¹ì–´ ì‹¤í–‰ ê²°ê³¼ ì¶œë ¥
                            if self.tts_system and (self.tts_system.engine or self.tts_system.use_sapi):
                                if action:
                                    # ëª…ë ¹ì–´ë³„ TTS ì‘ë‹µ ë©”ì‹œì§€
                                    tts_messages = {
                                        'light_on': 'ë„¤, ì „ë“±ì„ ì¼œë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                                        'light_off': 'ë„¤, ì „ë“±ì„ êº¼ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                                        'ac_on': 'ë„¤, ì—ì–´ì»¨ì„ ì¼œë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                                        'ac_off': 'ë„¤, ì—ì–´ì»¨ì„ êº¼ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                                        'fan_on': 'ë„¤, ì„ í’ê¸°ë¥¼ ì¼œë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                                        'fan_off': 'ë„¤, ì„ í’ê¸°ë¥¼ êº¼ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                                        'curtain_open': 'ë„¤, ì»¤íŠ¼ì„ ì—´ì–´ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                                        'curtain_close': 'ë„¤, ì»¤íŠ¼ì„ ë‹«ì•„ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                                        'tv_on': 'ë„¤, í‹°ë¹„ë¥¼ ì¼œë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                                        'tv_off': 'ë„¤, í‹°ë¹„ë¥¼ êº¼ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                                        'temp_up': 'ë„¤, ì˜¨ë„ë¥¼ ì˜¬ë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                                        'temp_down': 'ë„¤, ì˜¨ë„ë¥¼ ë‚´ë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                                        
                                        # ìƒˆë¡œìš´ ëª…ë ¹ì–´ TTS ë©”ì‹œì§€
                                        'ac_mode': 'ë„¤, ì—ì–´ì»¨ ëª¨ë“œë¥¼ ë³€ê²½í•˜ê² ìŠµë‹ˆë‹¤',
                                        'ac_power': 'ë„¤, ì—ì–´ì»¨ ì „ì›ì„ ì¡°ì‘í•˜ê² ìŠµë‹ˆë‹¤',
                                        'ac_tempDOWN': 'ë„¤, ì—ì–´ì»¨ ì˜¨ë„ë¥¼ ë‚®ì¶”ê² ìŠµë‹ˆë‹¤',
                                        'ac_tempUP': 'ë„¤, ì—ì–´ì»¨ ì˜¨ë„ë¥¼ ë†’ì´ê² ìŠµë‹ˆë‹¤',
                                        'tv_power': 'ë„¤, TV ì „ì›ì„ ì¡°ì‘í•˜ê² ìŠµë‹ˆë‹¤',
                                        'tv_channelUP': 'ë„¤, TV ì±„ë„ì„ ì˜¬ë¦¬ê² ìŠµë‹ˆë‹¤',
                                        'tv_channelDOWN': 'ë„¤, TV ì±„ë„ì„ ë‚´ë¦¬ê² ìŠµë‹ˆë‹¤',
                                        'spider_man': 'ë„¤, ìŠ¤íŒŒì´ë”ë§¨ ì œìŠ¤ì²˜ë¥¼ ì¸ì‹í–ˆìŠµë‹ˆë‹¤',
                                        'small_heart': 'ë„¤, ì‘ì€ í•˜íŠ¸ ì œìŠ¤ì²˜ë¥¼ ì¸ì‹í–ˆìŠµë‹ˆë‹¤',
                                        'thumbs_down': 'ë„¤, ì—„ì§€ ë‹¤ìš´ ì œìŠ¤ì²˜ë¥¼ ì¸ì‹í–ˆìŠµë‹ˆë‹¤',
                                        'thumbs_up': 'ë„¤, ì—„ì§€ ì—… ì œìŠ¤ì²˜ë¥¼ ì¸ì‹í–ˆìŠµë‹ˆë‹¤',
                                        'thumbs_left': 'ë„¤, ì—„ì§€ ì™¼ìª½ ì œìŠ¤ì²˜ë¥¼ ì¸ì‹í–ˆìŠµë‹ˆë‹¤',
                                        'thumbs_right': 'ë„¤, ì—„ì§€ ì˜¤ë¥¸ìª½ ì œìŠ¤ì²˜ë¥¼ ì¸ì‹í–ˆìŠµë‹ˆë‹¤'
                                    }
                                    
                                    tts_msg = tts_messages.get(action, f"ëª…ë ¹ì„ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤")
                                    self.tts_system.speak(tts_msg, async_mode=False)
                                else:
                                    self.tts_system.speak("ëª…ë ¹ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤", async_mode=False)
                    else:
                        print(" Colab URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            except KeyboardInterrupt:
                print(" ìŒì„± ì¸ì‹ ìŠ¤ë ˆë“œ ì¢…ë£Œ")
                break
            except Exception as e:
                print(f" ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {e}")
                time.sleep(1)
    
    def stop(self):
        """ìŠ¤ë ˆë“œ ì¤‘ì§€"""
        self.running = False
        if self.tts_system and (self.tts_system.engine or self.tts_system.use_sapi):
            self.tts_system.speak("ìŒì„± ì œì–´ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤", async_mode=False)


def main():
    """ë©”ì¸ í†µí•© í•¨ìˆ˜ (ì œìŠ¤ì²˜ + ìŒì„±)"""
    print(" í†µí•© ì œìŠ¤ì²˜ + ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ")
    print("test_integrated_gesture_live.py + voice_recognition.py")
    print("=" * 60)
    
    # ìŒì„± ì¸ì‹ ì„¤ì •
    colab_url = input("Colab ngrok URLì„ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒì‚¬í•­, Enterë¡œ ê±´ë„ˆë›°ê¸°): ").strip()
    if not colab_url:
        print(" Colab URL ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤. ìŒì„± ì¸ì‹ì€ ì œí•œì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
    
    # í†µí•© ì¸ì‹ê¸° ì´ˆê¸°í™” (test_integrated_gesture_live.pyì™€ ë™ì¼)
    recognizer = IntegratedGestureRecognizer(INTEGRATED_CONFIG)
    
    if not recognizer.mlp_model and not recognizer.tcn_model:
        print(" MLPì™€ TCN ëª¨ë¸ ëª¨ë‘ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    if not recognizer.mlp_model:
        print(" MLP ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë™ì  ì œìŠ¤ì²˜ë§Œ ì¸ì‹ë©ë‹ˆë‹¤.")
    
    if not recognizer.tcn_model:
        print(" TCN ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ì •ì  ì œìŠ¤ì²˜ë§Œ ì¸ì‹ë©ë‹ˆë‹¤.")
    
    # MediaPipe ì´ˆê¸°í™” (test_integrated_gesture_live.pyì™€ ë™ì¼)
    print(" MediaPipe ì´ˆê¸°í™” ì¤‘...")
    mp_hands = mp.solutions.hands
    hands = mp_hands.Hands(
        static_image_mode=False,
        max_num_hands=1,
        min_detection_confidence=INTEGRATED_CONFIG['min_detection_confidence'],
        min_tracking_confidence=INTEGRATED_CONFIG['min_tracking_confidence']
    )
    
    # ì›¹ìº  ì´ˆê¸°í™”
    print(" ì›¹ìº  ì´ˆê¸°í™” ì¤‘...")
    cap = cv2.VideoCapture(0)
    
    if not cap.isOpened():
        print(" ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    cap.set(cv2.CAP_PROP_FPS, 30)
    
    # ìŒì„± ì¸ì‹ ìŠ¤ë ˆë“œ ì‹œì‘
    print(" ìŒì„± ì¸ì‹ ìŠ¤ë ˆë“œ ì´ˆê¸°í™” ì¤‘...")
    voice_thread = VoiceRecognitionThread(colab_url)
    voice_thread.start()
    
    print(" ì´ˆê¸°í™” ì™„ë£Œ!")
    print("\n í†µí•© ì‹¤ì‹œê°„ ì¸ì‹ ì‹œì‘!")
    print(" ìœ ì—°í•œ ì¸ì‹ ë°©ì‹:")
    print("    ì •ì  ì œìŠ¤ì²˜: ê¸°ë³¸ ëª¨ë“œ - ì†ì„ 1ì´ˆê°„ ìœ ì§€í•˜ë©´ ì¦‰ì‹œ ì¸ì‹")
    print("    ë™ì  ì œìŠ¤ì²˜: 0.5ì´ˆ ì´ìƒ ì›€ì§ì„ â†’ 1ì´ˆ í›„ ì¸ì‹ (ì†ì´ í™”ë©´ì— ìˆì–´ë„ OK!)")
    print("    ìŒì„± ëª…ë ¹: 'ë¸Œë¦¿ì§€' + ëª…ë ¹ì–´ë¡œ IoT ì œì–´")
    if SOUND_AVAILABLE:
        print("    ì›¨ì´í¬ì›Œë“œ ì•Œë¦¼: 'ë -ë§-ë¡±' ë©œë¡œë”” ì¬ìƒ")
    else:
        print("    ì›¨ì´í¬ì›Œë“œ ì•Œë¦¼: ì‚¬ìš´ë“œ ì—†ìŒ (winsound ëª¨ë“ˆ í•„ìš”)")
    print("    ì›€ì§ì„ ì„ê³„ê°’: 0.04 (ë” ë¯¼ê°í•¨ - ì‹œê³„ë°©í–¥ íšŒì „ë„ ê°ì§€)")
    print("    ë¹ ë¥¸ ë°˜ì‘: 3-8í”„ë ˆì„ ìœˆë„ìš°ë¡œ ì¦‰ì‹œ ê°ì§€")
    print("   R - ìƒíƒœ ë¦¬ì…‹")
    print("   D - ë””ë²„ê·¸ ëª¨ë“œ í† ê¸€")
    print("   Q - ì¢…ë£Œ")
    print("=" * 60)
    print(" ì†ì„ ì¹´ë©”ë¼ì— ëŒ€ê³  ì œìŠ¤ì²˜ë¥¼ ìˆ˜í–‰í•˜ê±°ë‚˜ 'ë¸Œë¦¿ì§€' ìŒì„± ëª…ë ¹ì„ ë§í•˜ì„¸ìš”!")
    if SOUND_AVAILABLE:
        print(" ì›¨ì´í¬ì›Œë“œ 'ìŠ¤ë§ˆíŠ¸ ë¸Œë¦¿ì§€' ì¸ì‹ ì‹œ ì•Œë¦¼ìŒì´ ì¬ìƒë©ë‹ˆë‹¤!")
    
    fps_counter = deque(maxlen=30)
    frame_count = 0
    
    try:
        while True:
            start_time = time.time()
            
            ret, frame = cap.read()
            if not ret:
                print(" í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            frame = cv2.flip(frame, 1)
            frame_count += 1
            
            # ì† ëœë“œë§ˆí¬ ì¶”ì¶œ (test_integrated_gesture_live.pyì™€ ë™ì¼)
            landmarks, handedness, hand_confidence, finger_tip = extract_hand_landmarks(frame, hands)
            
            # ì¸ì‹ ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ (test_integrated_gesture_live.pyì™€ ë™ì¼)
            hand_detected = len(landmarks) > 0 and hand_confidence >= INTEGRATED_CONFIG['min_detection_confidence']
            
            if hand_detected:
                # MLPìš© íŠ¹ì§• ì¶”ì¶œ (test_existing_mlp_live.pyì™€ ë™ì¼)
                mlp_features = create_features_from_landmarks(landmarks)
                
                # TCNìš© íŠ¹ì§• ì¶”ì¶œ (collect_sequence_data.pyì™€ ë™ì¼)
                tcn_features = create_tcn_features_from_landmarks(landmarks)
                
                # ì¸ì‹ê¸°ì— ë‘ ê°€ì§€ íŠ¹ì§• ëª¨ë‘ ì „ë‹¬
                recognizer.add_frame(mlp_features, tcn_features, finger_tip, hand_detected=True)
                
                # í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
                status, _ = recognizer.get_status()
                
                # ì†ê³¼ ê¶¤ì  ê·¸ë¦¬ê¸°
                frame = draw_landmarks_and_trail(
                    frame, landmarks, finger_tip, recognizer.trail_points, 
                    handedness, hand_confidence, status
                )
            else:
                recognizer.add_frame(None, None, None, hand_detected=False)
            
            # ì˜ˆì¸¡ ì‹œë„ (test_integrated_gesture_live.pyì™€ ë™ì¼)
            recognizer.update_and_predict()
            
            # ì œìŠ¤ì²˜ ê²°ê³¼ ì„œë²„ ì „ì†¡ (ìƒˆë¡œìš´ ì¿¨ë‹¤ìš´ ì‹œìŠ¤í…œ ì ìš©)
            if recognizer.last_prediction is not None:
                gesture_name = ""
                if recognizer.prediction_source == "static":
                    gesture_name = recognizer.mlp_labels.get(recognizer.last_prediction, f'static_{recognizer.last_prediction}')
                else:
                    gesture_name = recognizer.tcn_labels.get(recognizer.last_prediction, f'dynamic_{recognizer.last_prediction}')
                
                print(f" [ì†ë™ì‘] ì œìŠ¤ì²˜ ì¸ì‹: {gesture_name} ({recognizer.prediction_source.upper()})")
                print(f" ì¸ì‹ ì‹œê°„: {time.strftime('%H:%M:%S')}")
                
                # nothing ì œìŠ¤ì²˜ëŠ” ì„œë²„ë¡œ ì „ì†¡í•˜ì§€ ì•ŠìŒ
                if gesture_name.lower() != 'nothing':
                    # ìƒˆë¡œìš´ ì¿¨ë‹¤ìš´ ì‹œìŠ¤í…œìœ¼ë¡œ ì „ì†¡
                    try_send_gesture(gesture_name)
                else:
                    print(" nothing ì œìŠ¤ì²˜ëŠ” ì„œë²„ë¡œ ì „ì†¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                
                # ì „ì†¡ í›„ ì˜ˆì¸¡ ê²°ê³¼ ì™„ì „ ì´ˆê¸°í™”
                recognizer.last_prediction = None
                recognizer.prediction_confidence = 0.0
                recognizer.prediction_source = ""
            
            # FPS ê³„ì‚° (test_integrated_gesture_live.pyì™€ ë™ì¼)
            end_time = time.time()
            fps = 1.0 / (end_time - start_time)
            fps_counter.append(fps)
            avg_fps = np.mean(fps_counter)
            
            # UI ê·¸ë¦¬ê¸° (test_integrated_gesture_live.pyì™€ ë™ì¼)
            frame = draw_integrated_ui(frame, recognizer, avg_fps)
            
            # í™”ë©´ í‘œì‹œ
            cv2.imshow('Unified Gesture + Voice Recognition System', frame)
            
            # í‚¤ ì…ë ¥ ì²˜ë¦¬ (test_integrated_gesture_live.pyì™€ ë™ì¼)
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                print("\n ì‚¬ìš©ìê°€ ì¢…ë£Œë¥¼ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.")
                break
            elif key == ord('r'):
                recognizer.reset_state()
                print(" ìƒíƒœ ë¦¬ì…‹")
            elif key == ord('d'):
                INTEGRATED_CONFIG['debug_mode'] = not INTEGRATED_CONFIG['debug_mode']
                print(f" ë””ë²„ê·¸ ëª¨ë“œ: {'ON' if INTEGRATED_CONFIG['debug_mode'] else 'OFF'}")
    
    except KeyboardInterrupt:
        print("\n ì¸í„°ëŸ½íŠ¸ë¡œ ì¢…ë£Œë©ë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\n ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ì •ë¦¬
        voice_thread.stop()
        voice_thread.join(timeout=3)
        cap.release()
        cv2.destroyAllWindows()
        hands.close()
        
        print("\n ì„¸ì…˜ í†µê³„:")
        if len(fps_counter) > 0:
            print(f"   - í‰ê·  FPS: {np.mean(fps_counter):.1f}")
        print(f"   - ì´ í”„ë ˆì„: {frame_count:,}")
        print("\n í†µí•© ì¸ì‹ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
