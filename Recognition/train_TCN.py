# train_TCN.py
"""
TCNì„ ì‚¬ìš©í•œ ì‹œí€€ìŠ¤ ì œìŠ¤ì²˜ ì¸ì‹ ëª¨ë¸ í•™ìŠµ
collect_sequence_data.pyë¡œ ìˆ˜ì§‘ëœ ë°ì´í„° ì‚¬ìš©

Author: AIoT Project Team
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os
import time
import pickle
from datetime import datetime
from collections import Counter
import glob

# =============================================================================
# ì„¤ì • ë° ìƒìˆ˜
# =============================================================================

# ì œìŠ¤ì²˜ ë¼ë²¨ ë§¤í•‘ (ìˆ˜ì§‘ ì½”ë“œì™€ ë™ì¼)
SEQUENCE_GESTURES = {
    'clockwise': 0,          # ì‹œê³„ë°©í–¥ ì›í˜•
    'counter_clockwise': 1,  # ë°˜ì‹œê³„ë°©í–¥ ì›í˜•
    'star': 2,              # ë³„ ëª¨ì–‘
    'triangle': 3,          # ì‚¼ê°í˜•
    'square': 4,            # ì‚¬ê°í˜•
    'heart': 5,             # í•˜íŠ¸ ëª¨ì–‘
    'spiral': 6,            # ë‚˜ì„ í˜•
    'zigzag': 7,            # ì§€ê·¸ì¬ê·¸
    'wave': 8,              # ë¬¼ê²°
    'infinity': 9           # ë¬´í•œëŒ€ ê¸°í˜¸
}

GESTURE_NAMES = list(SEQUENCE_GESTURES.keys())
LABEL_TO_NAME = {v: k for k, v in SEQUENCE_GESTURES.items()}

# í•™ìŠµ ì„¤ì •
TRAINING_CONFIG = {
    'data_dir': './gesture_data/new_sequence_data',
    'sequence_length': 60,          # ì‹œí€€ìŠ¤ ê¸¸ì´
    'input_features': 99,           # ì…ë ¥ íŠ¹ì§• ì°¨ì›
    'num_classes': 10,  # ê¸°ë³¸ê°’, ë°ì´í„° ë¡œë”© í›„ ì—…ë°ì´íŠ¸ë¨
    
    # TCN ì•„í‚¤í…ì²˜
    'tcn_channels': [32, 64, 128, 64, 32],  # TCN ì±„ë„ í¬ê¸°
    'kernel_size': 3,               # ì»¨ë³¼ë£¨ì…˜ ì»¤ë„ í¬ê¸°
    'dropout_rate': 0.3,            # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
    'use_skip_connections': True,   # ìŠ¤í‚µ ì—°ê²° ì‚¬ìš©
    'use_batch_norm': True,         # ë°°ì¹˜ ì •ê·œí™” ì‚¬ìš©
    
    # í•™ìŠµ ì„¤ì •
    'batch_size': 32,               # ë°°ì¹˜ í¬ê¸°
    'epochs': 150,                  # ì—í¬í¬ ìˆ˜
    'learning_rate': 0.001,         # í•™ìŠµë¥ 
    'weight_decay': 1e-4,           # ê°€ì¤‘ì¹˜ ê°ì‡ 
    'train_ratio': 0.7,             # í•™ìŠµ ë°ì´í„° ë¹„ìœ¨
    'val_ratio': 0.15,              # ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
    'test_ratio': 0.15,             # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
    
    # ìµœì í™” ì„¤ì •
    'early_stopping_patience': 20,  # ì¡°ê¸° ì¢…ë£Œ patience
    'lr_scheduler_patience': 10,     # í•™ìŠµë¥  ê°ì†Œ patience
    'lr_scheduler_factor': 0.5,      # í•™ìŠµë¥  ê°ì†Œ ë¹„ìœ¨
    'gradient_clip_value': 1.0,      # ê·¸ë¼ë””ì–¸íŠ¸ í´ë¦¬í•‘
    'class_balancing': True,         # í´ë˜ìŠ¤ ê· í˜• ë§ì¶”ê¸°
}

# =============================================================================
# TCN ëª¨ë¸ êµ¬í˜„
# =============================================================================

class Chomp1d(nn.Module):
    """TCNì„ ìœ„í•œ Chomp ë ˆì´ì–´ (ì¸ê³¼ê´€ê³„ ìœ ì§€)"""
    def __init__(self, chomp_size):
        super(Chomp1d, self).__init__()
        self.chomp_size = chomp_size

    def forward(self, x):
        return x[:, :, :-self.chomp_size].contiguous()

class TemporalBlock(nn.Module):
    """TCNì˜ ê¸°ë³¸ ë¸”ë¡"""
    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2, use_batch_norm=True):
        super(TemporalBlock, self).__init__()
        
        # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜
        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,
                               stride=stride, padding=padding, dilation=dilation)
        self.chomp1 = Chomp1d(padding)
        self.bn1 = nn.BatchNorm1d(n_outputs) if use_batch_norm else nn.Identity()
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)

        # ë‘ ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜
        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,
                               stride=stride, padding=padding, dilation=dilation)
        self.chomp2 = Chomp1d(padding)
        self.bn2 = nn.BatchNorm1d(n_outputs) if use_batch_norm else nn.Identity()
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)

        # ìŠ¤í‚µ ì—°ê²°ì„ ìœ„í•œ 1x1 ì»¨ë³¼ë£¨ì…˜
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        self.relu = nn.ReLU()
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.init_weights()

    def init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        self.conv1.weight.data.normal_(0, 0.01)
        self.conv2.weight.data.normal_(0, 0.01)
        if self.downsample is not None:
            self.downsample.weight.data.normal_(0, 0.01)

    def forward(self, x):
        # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡
        out = self.conv1(x)
        out = self.chomp1(out)
        out = self.bn1(out)
        out = self.relu1(out)
        out = self.dropout1(out)

        # ë‘ ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡
        out = self.conv2(out)
        out = self.chomp2(out)
        out = self.bn2(out)
        out = self.relu2(out)
        out = self.dropout2(out)

        # ìŠ¤í‚µ ì—°ê²°
        res = x if self.downsample is None else self.downsample(x)
        return self.relu(out + res)

class TemporalConvNet(nn.Module):
    """TCN ë„¤íŠ¸ì›Œí¬"""
    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2, use_batch_norm=True):
        super(TemporalConvNet, self).__init__()
        layers = []
        num_levels = len(num_channels)
        
        for i in range(num_levels):
            dilation_size = 2 ** i
            in_channels = num_inputs if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            padding = (kernel_size - 1) * dilation_size
            
            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,
                                   padding=padding, dropout=dropout, use_batch_norm=use_batch_norm)]

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

class SequenceTCN(nn.Module):
    """ì‹œí€€ìŠ¤ ì œìŠ¤ì²˜ ì¸ì‹ìš© TCN ëª¨ë¸"""
    def __init__(self, input_features, num_classes, tcn_channels, kernel_size=3, 
                 dropout_rate=0.3, use_skip_connections=True, use_batch_norm=True):
        super(SequenceTCN, self).__init__()
        
        self.input_features = input_features
        self.num_classes = num_classes
        self.use_skip_connections = use_skip_connections
        
        # ì…ë ¥ ì •ê·œí™”
        if use_batch_norm:
            self.input_norm = nn.BatchNorm1d(input_features)
        
        # TCN ë°±ë³¸
        self.tcn = TemporalConvNet(input_features, tcn_channels, kernel_size, dropout_rate, use_batch_norm)
        
        # Global pooling
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        # ë¶„ë¥˜ê¸°
        self.classifier = nn.Sequential(
            nn.Linear(tcn_channels[-1], tcn_channels[-1] // 2),
            nn.BatchNorm1d(tcn_channels[-1] // 2) if use_batch_norm else nn.Identity(),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(tcn_channels[-1] // 2, num_classes)
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._initialize_weights()
    
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # x shape: (batch, sequence_length, features)
        # TCN expects: (batch, features, sequence_length)
        x = x.transpose(1, 2)  # (batch, features, sequence_length)
        
        # ì…ë ¥ ì •ê·œí™”
        if hasattr(self, 'input_norm'):
            x = self.input_norm(x)
        
        # TCN í¬ì›Œë“œ
        tcn_out = self.tcn(x)  # (batch, channels, sequence_length)
        
        # Global pooling
        pooled = self.global_pool(tcn_out)  # (batch, channels, 1)
        pooled = pooled.squeeze(-1)  # (batch, channels)
        
        # ë¶„ë¥˜
        output = self.classifier(pooled)
        
        return output

# =============================================================================
# ë°ì´í„°ì…‹ í´ë˜ìŠ¤
# =============================================================================

class SequenceGestureDataset(Dataset):
    """ì‹œí€€ìŠ¤ ì œìŠ¤ì²˜ ë°ì´í„°ì…‹"""
    
    def __init__(self, sequences, labels):
        self.sequences = torch.FloatTensor(sequences)
        self.labels = torch.LongTensor(labels)
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        return self.sequences[idx], self.labels[idx]

# =============================================================================
# ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
# =============================================================================

def load_sequence_data(config):
    """ì‹œí€€ìŠ¤ ë°ì´í„° ë¡œë”©"""
    print("ğŸ“ ì‹œí€€ìŠ¤ ë°ì´í„° ë¡œë”© ì¤‘...")
    
    data_dir = config['data_dir']
    
    # .npz íŒŒì¼ë“¤ ì°¾ê¸°
    npz_files = glob.glob(os.path.join(data_dir, "sequence_gestures_*.npz"))
    
    if not npz_files:
        print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        print("ë¨¼ì € collect_sequence_data.pyë¥¼ ì‹¤í–‰í•´ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”.")
        return None, None, None, None
    
    # ê°€ì¥ ìµœì‹  íŒŒì¼ ì‚¬ìš©
    latest_file = max(npz_files, key=os.path.getctime)
    print(f"   ğŸ“ ë°ì´í„° íŒŒì¼: {os.path.basename(latest_file)}")
    
    try:
        # ë°ì´í„° ë¡œë”©
        data = np.load(latest_file, allow_pickle=True)
        sequences = data['sequences']           # (N, seq_len, features)
        gesture_names = data['gesture_names']   # (N,) - ê° ìƒ˜í”Œì˜ ì œìŠ¤ì²˜ ì´ë¦„
        unique_gestures = data['unique_gestures'].tolist()  # ìœ ë‹ˆí¬í•œ ì œìŠ¤ì²˜ ëª©ë¡
        gesture_to_label = data['gesture_to_label'].item()  # ì œìŠ¤ì²˜ -> ë¼ë²¨ ë§¤í•‘
        
        # ì œìŠ¤ì²˜ ì´ë¦„ì„ ë¼ë²¨ë¡œ ë³€í™˜
        labels = np.array([gesture_to_label[name] for name in gesture_names])
        
        # ê¸€ë¡œë²Œ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
        global SEQUENCE_GESTURES, LABEL_TO_NAME
        SEQUENCE_GESTURES = gesture_to_label
        LABEL_TO_NAME = {v: k for k, v in gesture_to_label.items()}
        
        print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ")
        print(f"   - ì´ ìƒ˜í”Œ: {len(sequences):,}")
        print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {sequences.shape[1]}")
        print(f"   - íŠ¹ì§• ì°¨ì›: {sequences.shape[2]}")
        print(f"   - í´ë˜ìŠ¤ ìˆ˜: {len(unique_gestures)}")
        print(f"   - ìˆ˜ì§‘ëœ ì œìŠ¤ì²˜: {unique_gestures}")
        
        # í´ë˜ìŠ¤ë³„ ë¶„í¬
        label_counts = Counter(labels)
        print(f"   - í´ë˜ìŠ¤ë³„ ë¶„í¬:")
        for label in sorted(label_counts.keys()):
            count = label_counts[label]
            gesture_name = LABEL_TO_NAME.get(label, f'unknown_{label}')
            percentage = count / len(labels) * 100
            print(f"     {label:2d} ({gesture_name:15s}): {count:4,} ({percentage:5.1f}%)")
        
        return sequences, labels, unique_gestures, gesture_to_label
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
        return None, None, None, None

def preprocess_sequences(sequences, labels, config):
    """ì‹œí€€ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬"""
    print("\nğŸ”„ ì‹œí€€ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    
    # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
    valid_mask = []
    for i, seq in enumerate(sequences):
        # NaN, inf ê²€ì‚¬
        if np.isnan(seq).any() or np.isinf(seq).any():
            valid_mask.append(False)
            continue
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ ê²€ì‚¬
        if len(seq) != config['sequence_length']:
            valid_mask.append(False)
            continue
        
        valid_mask.append(True)
    
    valid_mask = np.array(valid_mask)
    valid_sequences = sequences[valid_mask]
    valid_labels = labels[valid_mask]
    
    if valid_mask.sum() < len(sequences):
        print(f"   - ì œê±°ëœ ë¬´íš¨ ìƒ˜í”Œ: {(~valid_mask).sum():,}")
    
    print(f"   - ìœ íš¨í•œ ìƒ˜í”Œ: {len(valid_sequences):,}")
    
    # íŠ¹ì§• ì •ê·œí™” (ê° ìƒ˜í”Œì˜ ê° ì‹œì ë³„ë¡œ ì •ê·œí™”)
    print("   - íŠ¹ì§• ì •ê·œí™” ì¤‘...")
    
    # ëª¨ë“  ì‹œí€€ìŠ¤ë¥¼ í•˜ë‚˜ë¡œ í•©ì³ì„œ í†µê³„ ê³„ì‚°
    all_features = valid_sequences.reshape(-1, valid_sequences.shape[-1])
    scaler = StandardScaler()
    scaler.fit(all_features)
    
    # ê° ì‹œí€€ìŠ¤ ì •ê·œí™”
    normalized_sequences = np.zeros_like(valid_sequences)
    for i, seq in enumerate(valid_sequences):
        normalized_sequences[i] = scaler.transform(seq)
    
    print(f"   - ì •ê·œí™” ì™„ë£Œ")
    print(f"     ì›ë³¸ ë²”ìœ„: [{valid_sequences.min():.3f}, {valid_sequences.max():.3f}]")
    print(f"     ì •ê·œí™” í›„: [{normalized_sequences.min():.3f}, {normalized_sequences.max():.3f}]")
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
    class_weights = None
    if config['class_balancing']:
        label_counts = Counter(valid_labels)
        total_samples = len(valid_labels)
        n_classes = len(label_counts)
        
        weights = {}
        for label in range(config['num_classes']):
            count = label_counts.get(label, 1)
            weights[label] = total_samples / (n_classes * count)
        
        class_weights = torch.FloatTensor([weights[i] for i in range(config['num_classes'])])
        print(f"   - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ì™„ë£Œ")
    
    return normalized_sequences, valid_labels, scaler, class_weights

def create_data_loaders(sequences, labels, config):
    """ë°ì´í„°ë¡œë” ìƒì„±"""
    print("\nğŸ“¦ ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = SequenceGestureDataset(sequences, labels)
    
    # ë°ì´í„° ë¶„í• 
    total_size = len(dataset)
    train_size = int(total_size * config['train_ratio'])
    val_size = int(total_size * config['val_ratio'])
    test_size = total_size - train_size - val_size
    
    train_dataset, val_dataset, test_dataset = random_split(
        dataset, [train_size, val_size, test_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    # ë°ì´í„°ë¡œë” ìƒì„± (Windows í˜¸í™˜ì„±)
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config['batch_size'], 
        shuffle=True, 
        num_workers=0,  # Windows í˜¸í™˜ì„±
        pin_memory=False
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=config['batch_size'], 
        shuffle=False, 
        num_workers=0,
        pin_memory=False
    )
    test_loader = DataLoader(
        test_dataset, 
        batch_size=config['batch_size'], 
        shuffle=False, 
        num_workers=0,
        pin_memory=False
    )
    
    print(f"   - í•™ìŠµ: {len(train_dataset):,} ìƒ˜í”Œ")
    print(f"   - ê²€ì¦: {len(val_dataset):,} ìƒ˜í”Œ")
    print(f"   - í…ŒìŠ¤íŠ¸: {len(test_dataset):,} ìƒ˜í”Œ")
    
    return train_loader, val_loader, test_loader

# =============================================================================
# í•™ìŠµ í•¨ìˆ˜ë“¤
# =============================================================================

def train_epoch(model, train_loader, criterion, optimizer, device, config):
    """í•œ ì—í¬í¬ í•™ìŠµ"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for batch_idx, (data, targets) in enumerate(train_loader):
        data, targets = data.to(device), targets.to(device)
        
        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, targets)
        
        loss.backward()
        
        # ê·¸ë¼ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip_value'])
        
        optimizer.step()
        
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
    
    avg_loss = total_loss / len(train_loader)
    accuracy = 100. * correct / total
    
    return avg_loss, accuracy

def validate_epoch(model, val_loader, criterion, device):
    """ê²€ì¦ ì—í¬í¬"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for data, targets in val_loader:
            data, targets = data.to(device), targets.to(device)
            outputs = model(data)
            loss = criterion(outputs, targets)
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
    
    avg_loss = total_loss / len(val_loader)
    accuracy = 100. * correct / total
    
    return avg_loss, accuracy, all_predictions, all_targets

def train_model(model, train_loader, val_loader, config, device, class_weights=None):
    """ëª¨ë¸ í•™ìŠµ ë©”ì¸ í•¨ìˆ˜"""
    print("\nğŸš€ TCN ì‹œí€€ìŠ¤ ì œìŠ¤ì²˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
    print("=" * 60)
    
    # ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €
    if class_weights is not None:
        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))
        print("âš–ï¸ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©ë¨")
    else:
        criterion = nn.CrossEntropyLoss()
    
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=config['learning_rate'],
        weight_decay=config['weight_decay']
    )
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 
        mode='max', 
        factor=config['lr_scheduler_factor'], 
        patience=config['lr_scheduler_patience'], 
        verbose=True
    )
    
    # í•™ìŠµ ê¸°ë¡
    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': []
    }
    best_val_acc = 0
    best_model_state = None
    patience_counter = 0
    
    print(f"ğŸ”§ ëª¨ë¸ ì •ë³´:")
    print(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   - í•™ìŠµ ìƒ˜í”Œ: {len(train_loader.dataset):,}")
    print(f"   - ê²€ì¦ ìƒ˜í”Œ: {len(val_loader.dataset):,}")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {config['batch_size']}")
    print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {config['sequence_length']}")
    
    training_start_time = time.time()
    
    for epoch in range(config['epochs']):
        epoch_start = time.time()
        
        # í•™ìŠµ
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, device, config
        )
        
        # ê²€ì¦
        val_loss, val_acc, _, _ = validate_epoch(
            model, val_loader, criterion, device
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step(val_acc)
        
        epoch_time = time.time() - epoch_start
        
        # ê¸°ë¡ ì €ì¥
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_state = model.state_dict().copy()
            patience_counter = 0
            status = "ğŸ¯ NEW BEST!"
        else:
            patience_counter += 1
            status = f"({patience_counter}/{config['early_stopping_patience']})"
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if epoch % 5 == 0 or epoch == config['epochs'] - 1 or val_acc > best_val_acc:
            print(f"Epoch {epoch+1:3d}/{config['epochs']} | "
                  f"Train: {train_loss:.4f}/{train_acc:.2f}% | "
                  f"Val: {val_loss:.4f}/{val_acc:.2f}% | "
                  f"Time: {epoch_time:.1f}s | "
                  f"LR: {optimizer.param_groups[0]['lr']:.6f} | "
                  f"{status}")
        
        # ì¡°ê¸° ì¢…ë£Œ
        if patience_counter >= config['early_stopping_patience']:
            print(f"â¹ï¸ ì¡°ê¸° ì¢…ë£Œ (patience={config['early_stopping_patience']})")
            break
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
    model.load_state_dict(best_model_state)
    
    total_training_time = time.time() - training_start_time
    
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"   - ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.2f}%")
    print(f"   - ì´ í•™ìŠµ ì‹œê°„: {total_training_time/60:.1f}ë¶„")
    print(f"   - ì—í¬í¬ë‹¹ í‰ê· : {total_training_time/(epoch+1):.1f}ì´ˆ")
    
    return model, history

def evaluate_model(model, test_loader, device):
    """ëª¨ë¸ í‰ê°€"""
    print("\nğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
    
    criterion = nn.CrossEntropyLoss()
    test_loss, test_acc, predictions, targets = validate_epoch(
        model, test_loader, criterion, device
    )
    
    print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   - ì†ì‹¤: {test_loss:.4f}")
    print(f"   - ì •í™•ë„: {test_acc:.2f}%")
    
    # ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ
    target_names = [LABEL_TO_NAME.get(i, f'class_{i}') for i in range(TRAINING_CONFIG['num_classes'])]
    print(f"\nğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ:")
    print(classification_report(
        targets, predictions, 
        target_names=target_names,
        zero_division=0
    ))
    
    return test_acc, predictions, targets

def plot_results(history, predictions, targets, save_path='tcn_sequence_results.png'):
    """ê²°ê³¼ ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. í•™ìŠµ ê³¡ì„  (ì†ì‹¤)
    axes[0,0].plot(history['train_loss'], label='Train Loss', color='blue')
    axes[0,0].plot(history['val_loss'], label='Validation Loss', color='red')
    axes[0,0].set_title('Training and Validation Loss')
    axes[0,0].set_xlabel('Epoch')
    axes[0,0].set_ylabel('Loss')
    axes[0,0].legend()
    axes[0,0].grid(True)
    
    # 2. í•™ìŠµ ê³¡ì„  (ì •í™•ë„)
    axes[0,1].plot(history['train_acc'], label='Train Accuracy', color='blue')
    axes[0,1].plot(history['val_acc'], label='Validation Accuracy', color='red')
    axes[0,1].set_title('Training and Validation Accuracy')
    axes[0,1].set_xlabel('Epoch')
    axes[0,1].set_ylabel('Accuracy (%)')
    axes[0,1].legend()
    axes[0,1].grid(True)
    
    # 3. í˜¼ë™ í–‰ë ¬
    cm = confusion_matrix(targets, predictions)
    target_names = [LABEL_TO_NAME.get(i, f'class_{i}') for i in range(TRAINING_CONFIG['num_classes'])]
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í´ë˜ìŠ¤ë§Œ í‘œì‹œ
    existing_classes = sorted(list(set(targets + predictions)))
    existing_names = [target_names[i] for i in existing_classes]
    cm_existing = cm[np.ix_(existing_classes, existing_classes)]
    
    sns.heatmap(
        cm_existing, annot=True, fmt='d', cmap='Blues',
        xticklabels=existing_names, yticklabels=existing_names,
        ax=axes[1,0]
    )
    axes[1,0].set_title('Confusion Matrix')
    axes[1,0].set_xlabel('Predicted')
    axes[1,0].set_ylabel('True')
    
    # 4. í´ë˜ìŠ¤ë³„ ì •í™•ë„
    class_accuracies = []
    class_labels = []
    for i in existing_classes:
        mask = np.array(targets) == i
        if mask.sum() > 0:
            acc = accuracy_score(np.array(targets)[mask], np.array(predictions)[mask])
            class_accuracies.append(acc * 100)
            class_labels.append(LABEL_TO_NAME.get(i, f'{i}'))
    
    bars = axes[1,1].bar(range(len(class_accuracies)), class_accuracies)
    axes[1,1].set_title('Class-wise Accuracy')
    axes[1,1].set_xlabel('Class')
    axes[1,1].set_ylabel('Accuracy (%)')
    axes[1,1].set_xticks(range(len(class_labels)))
    axes[1,1].set_xticklabels(class_labels, rotation=45, ha='right')
    
    # ìƒ‰ìƒ ì½”ë”©
    for bar, acc in zip(bars, class_accuracies):
        if acc >= 95:
            bar.set_color('darkgreen')
        elif acc >= 90:
            bar.set_color('green')
        elif acc >= 80:
            bar.set_color('orange')
        else:
            bar.set_color('red')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"ğŸ“Š ê²°ê³¼ ê·¸ë˜í”„ê°€ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def save_model(model, scaler, config, test_accuracy, gesture_info):
    """ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸° ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ëª¨ë¸ ì €ì¥
    model_path = f'sequence_tcn_model_{test_accuracy:.1f}pct.pth'
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': config,
        'gesture_labels': SEQUENCE_GESTURES,
        'label_to_name': LABEL_TO_NAME,
        'unique_gestures': gesture_info['unique_gestures'],
        'gesture_to_label': gesture_info['gesture_to_label'],
        'model_class': 'SequenceTCN',
        'test_accuracy': test_accuracy,
        'timestamp': timestamp,
        'data_source': 'sequence_collected_data'
    }, model_path)
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    scaler_path = f'sequence_tcn_scaler.pkl'
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ:")
    print(f"   - ëª¨ë¸: {model_path}")
    print(f"   - ìŠ¤ì¼€ì¼ëŸ¬: {scaler_path}")
    print(f"   - í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.2f}%")
    print(f"   - í•™ìŠµëœ ì œìŠ¤ì²˜: {gesture_info['unique_gestures']}")

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¤– TCN ì‹œí€€ìŠ¤ ì œìŠ¤ì²˜ ì¸ì‹ ëª¨ë¸ í•™ìŠµ")
    print("collect_sequence_data.pyë¡œ ìˆ˜ì§‘ëœ ë°ì´í„° í™œìš©")
    print("=" * 60)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ë°ì´í„° ë¡œë”©
    sequences, labels, unique_gestures, gesture_to_label = load_sequence_data(TRAINING_CONFIG)
    if sequences is None:
        return
    
    # í´ë˜ìŠ¤ ìˆ˜ ì—…ë°ì´íŠ¸
    TRAINING_CONFIG['num_classes'] = len(unique_gestures)
    print(f"ğŸ”¢ í´ë˜ìŠ¤ ìˆ˜ ì—…ë°ì´íŠ¸: {TRAINING_CONFIG['num_classes']}")
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    sequences_scaled, labels, scaler, class_weights = preprocess_sequences(
        sequences, labels, TRAINING_CONFIG
    )
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader, val_loader, test_loader = create_data_loaders(
        sequences_scaled, labels, TRAINING_CONFIG
    )
    
    # ëª¨ë¸ ìƒì„±
    print(f"\nğŸ§  TCN ëª¨ë¸ ìƒì„±...")
    model = SequenceTCN(
        input_features=TRAINING_CONFIG['input_features'],
        num_classes=TRAINING_CONFIG['num_classes'],
        tcn_channels=TRAINING_CONFIG['tcn_channels'],
        kernel_size=TRAINING_CONFIG['kernel_size'],
        dropout_rate=TRAINING_CONFIG['dropout_rate'],
        use_skip_connections=TRAINING_CONFIG['use_skip_connections'],
        use_batch_norm=TRAINING_CONFIG['use_batch_norm']
    ).to(device)
    
    print(f"   - ì…ë ¥ íŠ¹ì§•: {TRAINING_CONFIG['input_features']}")
    print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {TRAINING_CONFIG['sequence_length']}")
    print(f"   - TCN ì±„ë„: {TRAINING_CONFIG['tcn_channels']}")
    print(f"   - ì¶œë ¥ í´ë˜ìŠ¤: {TRAINING_CONFIG['num_classes']}")
    print(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    
    # ëª¨ë¸ í•™ìŠµ
    trained_model, history = train_model(
        model, train_loader, val_loader, TRAINING_CONFIG, device, class_weights
    )
    
    # ëª¨ë¸ í‰ê°€
    test_accuracy, predictions, targets = evaluate_model(
        trained_model, test_loader, device
    )
    
    # ê²°ê³¼ ì‹œê°í™”
    plot_results(history, predictions, targets)
    
    # ëª¨ë¸ ì €ì¥
    gesture_info = {
        'unique_gestures': unique_gestures,
        'gesture_to_label': gesture_to_label
    }
    save_model(trained_model, scaler, TRAINING_CONFIG, test_accuracy, gesture_info)
    
    print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    print(f"   - ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.2f}%")
    print(f"   - ëª¨ë¸ íŒŒì¼: sequence_tcn_model_{test_accuracy:.1f}pct.pth")
    print(f"   - ë‹¤ìŒ ë‹¨ê³„: ì‹¤ì‹œê°„ í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ ì‘ì„±")

if __name__ == "__main__":

    main()
